{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Es-DcbFJGtQa",
        "outputId": "4635e1d6-5104-4ad1-bfd1-1e89ff37fb73"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'cpu'"
            ]
          },
          "execution_count": 223,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device='cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "HWDCJMiELmct"
      },
      "outputs": [],
      "source": [
        "jsonl_file_path = 'clips_dataset_wth_herbert_token_ids.jsonl'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "yy6ccgNNL4GC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming CUDA is available, models and data will be moved to GPU for faster processing.\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "input_size = 33 * 3  # 33 joints * 3 coordinates\n",
        "hidden_size = 256\n",
        "output_size = 50000 # Assuming a vocabulary size of 66\n",
        "embedding_size = 256  # Make embedding size equal to hidden_size for simplicity\n",
        "SOS_token = 0  # Start-of-Sequence token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "GKuYIHFV-RCE"
      },
      "outputs": [],
      "source": [
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {\"SOS\": 0, \"EOS\": 1}\n",
        "        self.word2count = {\"SOS\": 0, \"EOS\": 0}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2   # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "class SignLanguage:\n",
        "    def __init__(self):\n",
        "        self.frames = []\n",
        "        self.max_length = 0\n",
        "\n",
        "    def addFrames(self, frame_data):\n",
        "        self.frames.append(frame_data)\n",
        "        if len(frame_data) > self.max_length:\n",
        "            self.max_length = len(frame_data)\n",
        "\n",
        "    def getFrames(self):\n",
        "        return self.frames\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "Vpy9igWm-pwz"
      },
      "outputs": [],
      "source": [
        "def prepareData(jsonl_file_path, max_samples=1000):\n",
        "    data = []\n",
        "    with open(jsonl_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            data.append(json.loads(line))\n",
        "            if len(data) >= max_samples:\n",
        "                break\n",
        "\n",
        "    def processData(data):\n",
        "        sign_language = SignLanguage()\n",
        "        lang_annotations = Lang('pl')\n",
        "        for item in data:\n",
        "            sign_language.addFrames(item['FramesLandmarksCoords'])\n",
        "            lang_annotations.addSentence(item['PolishAnnotation'])\n",
        "        return sign_language, lang_annotations, [(item['FramesLandmarksCoords'], item['PolishAnnotation']) for item in data]\n",
        "\n",
        "    return processData(data)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yvqaZDJ_rjO",
        "outputId": "7c10caa2-1c7d-437f-9858-5a30c537f1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2000 pairs of sign language frames and Polish annotations.\n"
          ]
        }
      ],
      "source": [
        "training_dataset = prepareData(jsonl_file_path, max_samples=2000)\n",
        "\n",
        "train_sign_language, train_polish_annotations, train_pairs = training_dataset\n",
        "\n",
        "print(f\"Loaded {len(train_pairs)} pairs of sign language frames and Polish annotations.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "lwmM77OYAFGP"
      },
      "outputs": [],
      "source": [
        "EOS_token = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 230,
      "metadata": {
        "id": "wxqg7fvnB3Wh"
      },
      "outputs": [],
      "source": [
        "if 'EOS' not in train_polish_annotations.word2index:\n",
        "    train_polish_annotations.addWord('EOS')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 231,
      "metadata": {
        "id": "lL5PRPuf-t5d"
      },
      "outputs": [],
      "source": [
        "# from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# def tensorsFromPair(pair, annotations):\n",
        "#     # Attempt to handle a variety of data structures defensively\n",
        "#     try:\n",
        "#         # Flatten the frame data correctly, assuming it's deeply nested\n",
        "#         flat_list = [coord for frame in pair[0] for landmark in frame for coord in (landmark if isinstance(landmark, list) else [landmark])]\n",
        "#     except TypeError:\n",
        "#         # If there's a type error, log the problematic data and skip or handle appropriately\n",
        "#         print(\"Error processing pair:\", pair)\n",
        "#         flat_list = []\n",
        "\n",
        "#     # Now, calculate the dimensions based on the corrected flat list\n",
        "#     if flat_list:\n",
        "#         input_tensor = torch.tensor(flat_list, dtype=torch.float).view(-1, len(flat_list) // len(pair[0]))  # Adjust based on actual data length\n",
        "#     else:\n",
        "#         # Handle the error case (e.g., return a dummy tensor or skip this pair)\n",
        "#         input_tensor = torch.tensor([], dtype=torch.float).view(-1, 1)  # Dummy tensor, adjust as needed\n",
        "\n",
        "#     # Process target indexes\n",
        "#     target_indexes = [annotations.word2index[word] for word in pair[1].split(' ')] + [EOS_token]\n",
        "#     target_tensor = torch.tensor(target_indexes, dtype=torch.long)\n",
        "\n",
        "#     return (input_tensor, target_tensor)\n",
        "\n",
        "# from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# def get_dataloader(pairs, batch_size, annotations):\n",
        "#     input_tensors = []\n",
        "#     target_tensors = []\n",
        "\n",
        "#     for pair in pairs:\n",
        "#         input_tensor, target_tensor = tensorsFromPair(pair, annotations)\n",
        "#         input_tensors.append(input_tensor)\n",
        "#         target_tensors.append(target_tensor)\n",
        "\n",
        "#     input_tensors_padded = pad_sequence(input_tensors, batch_first=True, padding_value=0)\n",
        "#     target_tensors_padded = pad_sequence(target_tensors, batch_first=True, padding_value=annotations.word2index['EOS'])\n",
        "\n",
        "#     dataset = TensorDataset(input_tensors_padded, target_tensors_padded)\n",
        "    \n",
        "#     dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "#     return dataloader\n",
        "\n",
        "# # Function to split dataset and create dataloaders for each subset\n",
        "# def create_train_test_dataloaders(pairs, batch_size, annotations, split_ratio=0.5):\n",
        "#     # Calculate split sizes\n",
        "#     dataset_size = len(pairs)\n",
        "#     train_size = int(dataset_size * split_ratio)\n",
        "#     test_size = dataset_size - train_size\n",
        "\n",
        "#     # Split pairs dataset\n",
        "#     train_pairs, test_pairs = random_split(pairs, [train_size, test_size])\n",
        "\n",
        "#     # Create DataLoader for each subset\n",
        "#     train_loader = get_dataloader(list(train_pairs), batch_size, annotations)\n",
        "#     test_loader = get_dataloader(list(test_pairs), batch_size, annotations)\n",
        "\n",
        "#     return train_loader, test_loader\n",
        "\n",
        "# # Example usage\n",
        "# # Assuming 'train_pairs' is your dataset and 'annotations' is initialized\n",
        "# batch_size = 64\n",
        "# train_loader, test_loader = create_train_test_dataloaders(train_pairs, batch_size, train_polish_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "\n",
        "# Assuming EOS_token is defined somewhere\n",
        "EOS_token = 1  # Example value, adjust according to your EOS token's index\n",
        "\n",
        "def tensorsFromPair(pair, annotations):\n",
        "    try:\n",
        "        flat_list = [coord for frame in pair[0] for landmark in frame for coord in (landmark if isinstance(landmark, list) else [landmark])]\n",
        "    except TypeError:\n",
        "        print(\"Error processing pair:\", pair)\n",
        "        flat_list = []\n",
        "\n",
        "    if flat_list:\n",
        "        input_tensor = torch.tensor(flat_list, dtype=torch.float).view(-1, len(flat_list) // len(pair[0]))\n",
        "    else:\n",
        "        input_tensor = torch.tensor([], dtype=torch.float).view(-1, 1)\n",
        "\n",
        "    target_indexes = [annotations.word2index[word] for word in pair[1].split(' ')] + [EOS_token]\n",
        "    target_tensor = torch.tensor(target_indexes, dtype=torch.long)\n",
        "\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "def get_dataloader(pairs, batch_size, annotations, max_target_length):\n",
        "    input_tensors = []\n",
        "    target_tensors = []\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_tensor, target_tensor = tensorsFromPair(pair, annotations)\n",
        "        input_tensors.append(input_tensor)\n",
        "        # Manually pad target_tensor to max_target_length\n",
        "        padded_target_tensor = torch.cat([target_tensor, torch.full((max_target_length - target_tensor.size(0),), annotations.word2index['EOS'], dtype=torch.long)])\n",
        "        target_tensors.append(padded_target_tensor)\n",
        "\n",
        "    input_tensors_padded = pad_sequence(input_tensors, batch_first=True, padding_value=0)\n",
        "    target_tensors_padded = torch.stack(target_tensors, dim=0)  # Already manually padded\n",
        "\n",
        "    dataset = TensorDataset(input_tensors_padded, target_tensors_padded)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "def create_train_test_dataloaders(pairs, batch_size, annotations, split_ratio=0.8):\n",
        "    # Calculate the maximum target tensor length from all pairs\n",
        "    max_target_length = max(len(pair[1].split(' ')) for pair in pairs) + 1  # +1 for EOS_token\n",
        "\n",
        "    # Split the dataset\n",
        "    dataset_size = len(pairs)\n",
        "    train_size = int(dataset_size * split_ratio)\n",
        "    test_size = dataset_size - train_size\n",
        "    train_pairs, test_pairs = random_split(pairs, [train_size, test_size])\n",
        "\n",
        "    # Create DataLoader for each subset, now passing max_target_length\n",
        "    train_loader = get_dataloader(list(train_pairs), batch_size, annotations, max_target_length)\n",
        "    test_loader = get_dataloader(list(test_pairs), batch_size, annotations, max_target_length)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "# Example usage:\n",
        "# Assuming 'train_pairs' is your dataset and 'annotations' is initialized with a 'word2index' mapping\n",
        "batch_size = 64\n",
        "train_loader, test_loader = create_train_test_dataloaders(train_pairs, batch_size, train_polish_annotations)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 233,
      "metadata": {
        "id": "A32MG1ghC0D7"
      },
      "outputs": [],
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Applying dropout to the input might not be directly applicable if input is continuous.\n",
        "        # Consider applying dropout after GRU if needed or using it in a different manner.\n",
        "        output, hidden = self.gru(input)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 234,
      "metadata": {},
      "outputs": [],
      "source": [
        "MAX_LEN = 0\n",
        "\n",
        "for input_tensor, target_tensor in train_loader:\n",
        "    if target_tensor.size()[1] > MAX_LEN:\n",
        "        MAX_LEN = target_tensor.size()[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 235,
      "metadata": {
        "id": "fxJ6QOXEC8gx"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "\n",
        "        for i in range(MAX_LEN):\n",
        "            decoder_output, decoder_hidden  = self.forward_step(decoder_input, decoder_hidden)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            if target_tensor is not None and i < target_tensor.size(1) - 1:\n",
        "            # Teacher forcing: Feed the target as the next input, ensuring we don't exceed target_tensor's length\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs, decoder_hidden, None # We return `None` for consistency in the training loop\n",
        "\n",
        "    def forward_step(self, input, hidden):\n",
        "        output = self.embedding(input)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.out(output)\n",
        "        return output, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {
        "id": "hDw3oBe2EzY5"
      },
      "outputs": [],
      "source": [
        "class BahdanauAttention(nn.Module):\n",
        "    def __init__(self, hidden_size):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
        "        self.Va = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, query, keys):\n",
        "        query = query.float()\n",
        "        keys = keys.float()\n",
        "        scores = self.Va(torch.tanh(self.Wa(query.expand(-1, keys.size(1), -1)) + self.Ua(keys)))\n",
        "        scores = scores.squeeze(2).unsqueeze(1)\n",
        "\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "        context = torch.bmm(weights, keys)\n",
        "\n",
        "        return context, weights\n",
        "\n",
        "\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.attention = BahdanauAttention(hidden_size)\n",
        "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
        "        batch_size = encoder_outputs.size(0)\n",
        "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_outputs = []\n",
        "        attentions = []\n",
        "\n",
        "        for i in range(MAX_LEN):\n",
        "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            decoder_outputs.append(decoder_output)\n",
        "            attentions.append(attn_weights)\n",
        "\n",
        "            if target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        attentions = torch.cat(attentions, dim=1)\n",
        "\n",
        "        return decoder_outputs, decoder_hidden, attentions\n",
        "\n",
        "\n",
        "    def forward_step(self, input, hidden, encoder_outputs):\n",
        "        embedded =  self.dropout(self.embedding(input))\n",
        "\n",
        "        query = hidden.permute(1, 0, 2)\n",
        "        context, attn_weights = self.attention(query, encoder_outputs)\n",
        "        input_gru = torch.cat((embedded, context), dim=2)\n",
        "\n",
        "        output, hidden = self.gru(input_gru, hidden)\n",
        "        output = self.out(output)\n",
        "\n",
        "        return output, hidden, attn_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 237,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsTsP17AFsR8",
        "outputId": "21c4c880-e864-4648-a4f0-5ee8f96d1dfb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EncoderRNN(\n",
            "  (gru): GRU(99, 256, batch_first=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n",
            "AttnDecoderRNN(\n",
            "  (embedding): Embedding(4515, 256)\n",
            "  (attention): BahdanauAttention(\n",
            "    (Wa): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (Ua): Linear(in_features=256, out_features=256, bias=True)\n",
            "    (Va): Linear(in_features=256, out_features=1, bias=True)\n",
            "  )\n",
            "  (gru): GRU(512, 256, batch_first=True)\n",
            "  (out): Linear(in_features=256, out_features=4515, bias=True)\n",
            "  (dropout): Dropout(p=0.1, inplace=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "input_size = 33 * 3  # Assuming each frame is represented by 33 landmarks, each with 3 coordinates\n",
        "hidden_size = 256  # Example hidden size, adjust based on model complexity and dataset\n",
        "output_size = len(train_polish_annotations.word2index)\n",
        "dropout_p = 0.1  # Dropout probability\n",
        "\n",
        "encoder = EncoderRNN(input_size, hidden_size, dropout_p).to(device)\n",
        "decoder = AttnDecoderRNN(hidden_size, output_size, dropout_p).to(device)\n",
        "\n",
        "# Example of model initialization\n",
        "print(encoder)\n",
        "print(decoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {
        "id": "ITN7A14zFw-A"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def decode_sentence(indices, lang):\n",
        "    return ' '.join([lang.index2word.get(index, '') for index in indices if index not in (EOS_token,)])\n",
        "\n",
        "def train_epoch(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for input_tensor, target_tensor in dataloader:\n",
        "        input_tensor = input_tensor.to(device)\n",
        "        target_tensor = target_tensor.to(device)\n",
        "\n",
        "        encoder_optimizer.zero_grad()\n",
        "        decoder_optimizer.zero_grad()\n",
        "\n",
        "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        encoder_optimizer.step()\n",
        "        decoder_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy excluding padding\n",
        "        _, predicted = decoder_outputs.max(-1)  # Get the index of the max log-probability\n",
        "        non_padding_mask = target_tensor != EOS_token\n",
        "        correct = (predicted == target_tensor) & non_padding_mask  # Apply mask\n",
        "        num_correct = correct.sum().item()\n",
        "        total_samples += non_padding_mask.sum().item()  # Count non-padding values only\n",
        "\n",
        "        total_correct += num_correct\n",
        "\n",
        "    # Select a random example from the last batch processed for printing\n",
        "    example_index = random.randint(0, input_tensor.size(0) - 1)  # Randomly select an index\n",
        "    predicted_example = predicted[example_index]\n",
        "    target_example = target_tensor[example_index]\n",
        "\n",
        "    predicted_sentence = decode_sentence(predicted_example[predicted_example != EOS_token].cpu().numpy(), train_polish_annotations)\n",
        "    target_sentence = decode_sentence(target_example[target_example != EOS_token].cpu().numpy(), train_polish_annotations)\n",
        "    \n",
        "    print(\"Predicted sentence:\", predicted_sentence)\n",
        "    print(\"Target sentence:\", target_sentence)\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
        "\n",
        "    return average_loss, accuracy\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 239,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "xGJq-GXFGQYA",
        "outputId": "172cb45c-ea70-43c6-9778-02a177bda9fe"
      },
      "outputs": [],
      "source": [
        "encoder_optimizer = optim.SGD(encoder.parameters(), lr=0.0001)\n",
        "decoder_optimizer = optim.SGD(decoder.parameters(), lr=0.0001)\n",
        "criterion = nn.NLLLoss(ignore_index=train_polish_annotations.word2index['EOS']).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 240,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 241,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(dataloader, encoder, decoder, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    \n",
        "    with torch.no_grad():  # Disable gradient calculation\n",
        "        for input_tensor, target_tensor in dataloader:\n",
        "            input_tensor = input_tensor.to(device)\n",
        "            target_tensor = target_tensor.to(device)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
        "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
        "\n",
        "            loss = criterion(decoder_outputs.view(-1, decoder_outputs.size(-1)), target_tensor.view(-1))\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy excluding padding\n",
        "            _, predicted = decoder_outputs.max(-1)  # Get the index of the max log-probability\n",
        "            non_padding_mask = target_tensor != EOS_token\n",
        "            correct = (predicted == target_tensor) & non_padding_mask  # Apply mask\n",
        "            num_correct = correct.sum().item()\n",
        "            total_samples += non_padding_mask.sum().item()  # Count non-padding values only\n",
        "\n",
        "            total_correct += num_correct\n",
        "\n",
        "    average_loss = total_loss / len(dataloader)\n",
        "    accuracy = total_correct / total_samples if total_samples > 0 else 0\n",
        "    \n",
        "    return average_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 242,
      "metadata": {
        "id": "O_79injAF58U"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n",
        "\n",
        "def train(train_dataloader, test_dataloader, encoder, decoder, n_epochs, learning_rate=0.001, print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0\n",
        "    plot_loss_total = 0\n",
        "    print_acc_total = 0\n",
        "\n",
        "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss().to(device)\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss, accuracy = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        print_acc_total += accuracy\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        test_loss, test_accuracy = evaluate(test_dataloader, encoder, decoder, criterion)\n",
        "        \n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_acc_avg = print_acc_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print_acc_total = 0\n",
        "            print('%s (%d %d%%) Train Loss: %.4f Train Acc: %.2f%% Test Loss: %.4f Test Acc: %.2f%%' % (timeSince(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg, print_acc_avg * 100, test_loss, test_accuracy * 100))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    print(plot_losses)\n",
        "    showPlot(plot_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 244,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 467, 99]) torch.Size([64, 42])\n",
            "torch.Size([64, 467, 99]) torch.Size([64, 42])\n",
            "torch.Size([64, 467, 99]) torch.Size([64, 42])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[244], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_every\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[242], line 29\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_dataloader, encoder, decoder, n_epochs, learning_rate, print_every, plot_every)\u001b[0m\n\u001b[0;32m     26\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mNLLLoss()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 29\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     print_loss_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     31\u001b[0m     print_acc_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy\n",
            "Cell \u001b[1;32mIn[238], line 27\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(decoder_outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, decoder_outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)), target_tensor\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 27\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m encoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m decoder_optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
            "File \u001b[1;32mc:\\Users\\Iga Miller\\OneDrive\\Pulpit\\Studia_mgr_SI\\Semestr2\\Analiza_mediow_cyfrowych\\YT_miniatures\\.venv\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\Iga Miller\\OneDrive\\Pulpit\\Studia_mgr_SI\\Semestr2\\Analiza_mediow_cyfrowych\\YT_miniatures\\.venv\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train(train_loader,test_loader, encoder, decoder, 15, print_every=1, plot_every=1)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
