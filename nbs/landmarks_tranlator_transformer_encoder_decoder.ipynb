{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Landmarks Sequence (Sign Language) to Polish Tranlation\n",
    "> Architecture: **Encoder-Decoder** with **Attention** Mechanism, [**Transformer**](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn, Tensor, functional as F\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Any, Callable, Optional\n",
    "from jsonlines import jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "from loguru import logger\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "from src.settings import PREPROCESSED_DIR, MODELS_DIR, LOGS_DIR\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, records: List[Dict[str, Any]]):\n",
    "        self.records = records\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        out_polish_token_ids = torch.tensor(self.records[index][\"PolishAnnotationTokenIds\"], dtype=torch.int32)\n",
    "        if len(out_polish_token_ids) > 66:\n",
    "            # FIXME: this is a hack to make it work with the model; only one clip is affected\n",
    "            print(\"Warning: PolishAnnotationTokenIds is longer than 66\")\n",
    "            out_polish_token_ids = out_polish_token_ids[:66]\n",
    "\n",
    "        # (seq_len x 33 x 3) -> (seq_len x 99)\n",
    "        frame_seq_landmarks = torch.tensor(self.records[index][\"FramesLandmarksCoords\"], dtype=torch.float32).view(-1, 99)\n",
    "        prepro_landmarks_seq = self.preprocess_landmarks_seq(frame_seq_landmarks)\n",
    "\n",
    "        return {\n",
    "            \"in_landmarks\": prepro_landmarks_seq,\n",
    "            \"out_polish_token_ids\": out_polish_token_ids,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_landmarks_seq(landmarks_seq, long_multiplier=10_000):\n",
    "        # hack for nn.Embedding layer to work; it requires long (int) type\n",
    "        x = landmarks_seq * long_multiplier\n",
    "        x = landmarks_seq + torch.min(landmarks_seq) * -1\n",
    "        return x.long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Clips Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_FRAC = 0.001\n",
    "\n",
    "\n",
    "with jsonlines.open(PREPROCESSED_DIR / \"clips_dataset_wth_herbert_token_ids.jsonl\") as reader:\n",
    "    # total_records: 19_503\n",
    "    if SAMPLE_FRAC < 1:\n",
    "        raw_records = list((rec for rec in reader if np.random.choice([True, False], p=[SAMPLE_FRAC, 1 - SAMPLE_FRAC])))  # iterable approach for random sample\n",
    "    else:\n",
    "        raw_records = list(reader)\n",
    "\n",
    "train_records, val_records = train_test_split(raw_records, test_size=0.2)\n",
    "\n",
    "train_ds, val_ds = ClipsDataset(train_records), ClipsDataset(val_records)\n",
    "\n",
    "del train_records, val_records, raw_records\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clips_df = pd.DataFrame.from_records(raw_records)\n",
    "# clips_df.FramesLandmarksCoords.apply(lambda x: np.array(x).flatten()).explode().astype(\"float\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([record[\"NumFrames\"] for record in raw_records]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([record[\"PolishAnnotationTokenIds\"] for record in raw_records]).map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 99])\n",
      "torch.Size([66])\n"
     ]
    }
   ],
   "source": [
    "for record in train_ds:\n",
    "    print(record[\"in_landmarks\"].shape)  # n_frames x n_landmarks*3\n",
    "    print(record[\"out_polish_token_ids\"].shape)  # padded n_tokens\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LANDMARKS = 33\n",
    "COORD_CHANNELS = 3\n",
    "MAX_TOKENS = 66\n",
    "MAX_FRAMES = 392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/#:~:text=What%20Is%20Positional%20Encoding%3F\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # max_len x 1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # d_model/2\n",
    "        pe = torch.zeros(max_len, 1, 1, d_model)  # max_len x 99 x 1 x d_model\n",
    "        # pe = torch.zeros(max_len, 1, d_model)  # max_len x 99 x 1 x d_model\n",
    "        # print(f\"{position.shape=}\\n{div_term.shape=}\\n{pe.shape=}\")\n",
    "        pe[:, 0, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 0, 1::2] = torch.cos(position * div_term)\n",
    "        # pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        # pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[batch_len, 99, BATCH_SIZE, EMBEDDING_LAYER_OUT_SIZE]``\n",
    "\n",
    "        Returns:\n",
    "            Tensor, (shape like ``x``): added positional encoding to ``x``\n",
    "        \"\"\"\n",
    "        # x : batch_len, 99, batch_size, d_model\n",
    "        print(f\"{x.shape=}, {self.pe.shape=}\")\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksSeqTransformerEncoder(nn.Module):\n",
    "    # https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, device: torch.device = torch.device(\"cuda:0\")) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "\n",
    "        # _, _, batch_size, d_model = src.shape\n",
    "        # src = src.view(-1, batch_size, d_model)\n",
    "\n",
    "        src = self.pos_encoder(src)  # batch_len, 99, batch_size, d_model\n",
    "\n",
    "        _, _, batch_size, d_model = src.shape\n",
    "        src = src.view(-1, batch_size, d_model)\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src), device=self.device)\n",
    "        # print(f\"{src.shape=}\\n{src_mask.shape=}\")\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksSeqTransformerDecoder(nn.Module):\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, device: torch.device = torch.device(\"cuda:0\")) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor = None,\n",
    "                memory_mask: torch.Tensor = None, tgt_key_padding_mask: torch.Tensor = None,\n",
    "                memory_key_padding_mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        print(f\"{tgt.shape=}\")\n",
    "\n",
    "        # batch_size, d_model = tgt.shape\n",
    "        # tgt = tgt.view(-1, batch_size, d_model)\n",
    "\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "\n",
    "        _, _, batch_size, d_model = tgt.shape\n",
    "        tgt = tgt.view(-1, batch_size, d_model)\n",
    "\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = nn.Transformer.generate_square_subsequent_mask(len(tgt)).to(self.device)\n",
    "\n",
    "        print(f\"{tgt.shape=} {memory.shape=} {tgt_mask.shape=}\")\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                          memory_key_padding_mask=memory_key_padding_mask)\n",
    "        # output: \n",
    "        output = self.linear(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(dataset: Dataset, batch_size: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``batch_size`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        dataset: Dataset\n",
    "        batch_size: int\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "\n",
    "    # to work wth dataset:\n",
    "    data = torch.cat([record[\"in_landmarks\"] for record in dataset])\n",
    "\n",
    "    # oryginalnie data: (20857)\n",
    "\n",
    "    # clips_landmarks_seq = [record[\"in_landmarks\"] for record in data]\n",
    "    # print(clips_landmarks_seq.shape)\n",
    "    # data = torch.tensor(clips_landmarks_seq)\n",
    "\n",
    "    batch_len = data.size(0) // batch_size\n",
    "    # print(seq_len)\n",
    "    # data = data.view(batch_size, seq_len)\n",
    "    data = data[:batch_len * batch_size , :]  # batch_len x 99\n",
    "    # print(data.shape)\n",
    "\n",
    "    data = data.view(batch_len, -1, batch_size)\n",
    "    # data = data.t()\n",
    "    # print(data.shape, data)\n",
    "    data = data.contiguous()  # batch_len x 99 x batch_size\n",
    "    # print(data.shape, data)\n",
    "    print(f\"{data.device=}\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_batch(\n",
    "    batched_data_dl: Tensor,\n",
    "    step: int,\n",
    "    bptt: int,\n",
    "    device: torch.device = torch.device(\"cuda:0\")\n",
    ") -> (Tensor, Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        step: int\n",
    "        bbpt: int, batch_len\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "\n",
    "    query_batch_len = min(bptt, len(batched_data_dl) - 1 - step)\n",
    "    data = batched_data_dl[step:step+query_batch_len, :, :]\n",
    "    target = batched_data_dl[step+1:step+1+query_batch_len].reshape(-1)\n",
    "    return data.to(device), target.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.device=device(type='cpu')\n",
      "data.device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "BPTT = 3\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dl = batchify(train_ds, batch_size=BATCH_SIZE)\n",
    "val_dl = batchify(val_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dl.shape=torch.Size([325, 99, 4])\n",
      "data.shape=torch.Size([3, 99, 4])\n",
      "targets.shape=torch.Size([1188])\n"
     ]
    }
   ],
   "source": [
    "# print(train_ds[0][\"in_landmarks\"].shape)\n",
    "\n",
    "# data = torch.cat([record[\"in_landmarks\"] for record in train_ds])\n",
    "# print(data.shape, data, \"\\n\")\n",
    "print(f\"{train_dl.shape=}\")\n",
    "\n",
    "for batch, step in enumerate(range(0, train_dl.size(0) - 1, BPTT)):\n",
    "    if batch == 1:\n",
    "        break\n",
    "    # print(input_.shape)\n",
    "    data, targets = get_batch(train_dl, step, bptt=BPTT)\n",
    "    print(f\"{data.shape=}\")\n",
    "    print(f\"{targets.shape=}\")\n",
    "\n",
    "del train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 50_000  # aka lenght of vector that contains probabilities for each token; must match the tokenizer `token_ids` range (0 included)\n",
    "DROPOUT = 0.2  # dropout probability\n",
    "EMBEDDING_LAYER_OUT_SIZE = 200  # embedding dimension\n",
    "\n",
    "ENCODING_TRANSFORMER_LAYERS = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "ENCODING_TRANSFORMER_HIDDEN_DIM = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "ENCODING_TRANSFORMER_NHEAD = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "\n",
    "\n",
    "encoder = LandmarksSeqTransformerEncoder(\n",
    "    ntoken=VOCAB_SIZE,\n",
    "    d_model=EMBEDDING_LAYER_OUT_SIZE,\n",
    "    nhead=ENCODING_TRANSFORMER_NHEAD,\n",
    "    d_hid=ENCODING_TRANSFORMER_HIDDEN_DIM,\n",
    "    nlayers=ENCODING_TRANSFORMER_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "decoder = LandmarksSeqTransformerDecoder(\n",
    "    ntoken=VOCAB_SIZE,\n",
    "    d_model=EMBEDDING_LAYER_OUT_SIZE,\n",
    "    nhead=ENCODING_TRANSFORMER_NHEAD,\n",
    "    d_hid=ENCODING_TRANSFORMER_HIDDEN_DIM,\n",
    "    nlayers=ENCODING_TRANSFORMER_LAYERS,\n",
    "    dropout=DROPOUT,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "opt_params = set(encoder.parameters()) | set(decoder.parameters())\n",
    "optimizer = optim.SGD(opt_params, lr=LEARNING_RATE)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Checkpoint Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_model_name(model: nn.Module, params: Dict[str, int | float], unique_suffix: Optional[str] = None) -> str:\n",
    "    if unique_suffix is None:\n",
    "        unique_suffix = datetime.strftime(datetime.now(), '%Y%m%d_%H%M%Sf%f')\n",
    "    params_safe_str = \"__\".join([\n",
    "        f\"{key}{f'{v:.3f}'.replace('.', 'f') if isinstance(v, float) else v}\"\n",
    "        for key, v in params.items()\n",
    "    ])\n",
    "    return f\"{model.__class__.__name__}__{params_safe_str}__{unique_suffix}.best.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LandmarksSeqTransformerEncoder__sample0f001__lr5__vocab50000__epochs5__20240318_203130f712186.best.pt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_model_name = get_safe_model_name(encoder, params={\n",
    "    \"sample\": SAMPLE_FRAC,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"vocab\": VOCAB_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "})\n",
    "\n",
    "best_encoder_params_path = MODELS_DIR / safe_model_name\n",
    "best_encoder_params_path.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redirect std.out to file\n",
    "# logger.remove(0)\n",
    "_ = logger.add(LOGS_DIR / f\"{safe_model_name}.log\", backtrace=True, diagnose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    encoder: nn.Module,\n",
    "    decoder: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    lr_scheduler: optim.lr_scheduler.LRScheduler,\n",
    "    loss_fn: nn.Module,\n",
    "    train_dataloader: Tensor,\n",
    "    epoch: int,\n",
    "    logger: logger.__class__,\n",
    "    callback: Callable,\n",
    "    cb_kwargs: dict\n",
    ") -> None:\n",
    "\n",
    "    encoder.train()  # turn on train mode\n",
    "    decoder.train()  # turn on train mode\n",
    "\n",
    "    total_encoder_loss = 0.0\n",
    "    start_time = time.time()\n",
    "    num_batches = len(train_dataloader) // BPTT\n",
    "    log_interval = num_batches // 10\n",
    "    num_steps = train_dataloader.size(0) - 1\n",
    "    for batch, step in callback(enumerate(range(0, num_steps, BPTT)), total=num_batches, **cb_kwargs):\n",
    "        data, targets = get_batch(train_dataloader, step, BPTT, device=DEVICE)\n",
    "\n",
    "        encoder_output = encoder(data)\n",
    "        encoder_output_flat = encoder_output.view(-1, VOCAB_SIZE)\n",
    "        encoder_loss = loss_fn(encoder_output_flat, targets)\n",
    "\n",
    "        print(f\"{encoder_output.shape=} {encoder_output_flat.shape=}\")\n",
    "        decoder_output = decoder(targets, encoder_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        encoder_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(encoder.parameters(), 0.5)\n",
    "        nn.utils.clip_grad_norm_(decoder.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_encoder_loss += encoder_loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = lr_scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_encoder_loss = total_encoder_loss / log_interval\n",
    "            ppl = math.exp(cur_encoder_loss)\n",
    "            logger.info(f'  epoch {epoch:3d} | {batch:5d}/{num_batches} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'encoder loss {cur_encoder_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_encoder_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    eval_dataloader: Tensor,\n",
    "    callback: Callable,\n",
    "    cb_kwargs: dict\n",
    ") -> float:\n",
    "\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    dl_len = len(eval_dataloader)\n",
    "    num_batches = dl_len // BPTT\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in callback(range(0, eval_dataloader.size(0) - 1, BPTT), total=num_batches, **cb_kwargs):\n",
    "            data, targets = get_batch(eval_dataloader, i, device=DEVICE)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, VOCAB_SIZE)\n",
    "            total_loss += seq_len * loss_fn(output_flat, targets).item()\n",
    "    return total_loss / (dl_len - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "783af168c9f9436f9310f8ffe9982c08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4769d343a9f8465696361c436871144f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training | Batches:   0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape=torch.Size([3, 99, 4, 200]), self.pe.shape=torch.Size([5000, 1, 1, 200])\n",
      "encoder_output.shape=torch.Size([297, 4, 50000]) encoder_output_flat.shape=torch.Size([1188, 50000])\n",
      "tgt.shape=torch.Size([1188, 200])\n",
      "x.shape=torch.Size([1188, 200]), self.pe.shape=torch.Size([5000, 1, 1, 200])\n",
      "tgt.shape=torch.Size([1188, 1188, 200]) memory.shape=torch.Size([297, 4, 50000]) tgt_mask.shape=torch.Size([1188, 1188])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1188x50000 and 200x400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, EPOCHS \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpochs\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     epoch_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 11\u001b[0m     train_epoch(\n\u001b[1;32m     12\u001b[0m         encoder\u001b[39m=\u001b[39;49mencoder,\n\u001b[1;32m     13\u001b[0m         decoder\u001b[39m=\u001b[39;49mdecoder,\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[39m=\u001b[39;49moptimizer,\n\u001b[1;32m     15\u001b[0m         lr_scheduler\u001b[39m=\u001b[39;49mscheduler,\n\u001b[1;32m     16\u001b[0m         loss_fn\u001b[39m=\u001b[39;49mcriterion,\n\u001b[1;32m     17\u001b[0m         train_dataloader\u001b[39m=\u001b[39;49mtrain_dl,\n\u001b[1;32m     18\u001b[0m         epoch\u001b[39m=\u001b[39;49mepoch,\n\u001b[1;32m     19\u001b[0m         logger\u001b[39m=\u001b[39;49mlogger,\n\u001b[1;32m     20\u001b[0m         callback\u001b[39m=\u001b[39;49mtqdm,\n\u001b[1;32m     21\u001b[0m         cb_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mdesc\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mTraining | Batches\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mposition\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m0\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mleave\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mTrue\u001b[39;49;00m}\n\u001b[1;32m     22\u001b[0m     )\n\u001b[1;32m     23\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(\n\u001b[1;32m     24\u001b[0m         model\u001b[39m=\u001b[39mencoder,\n\u001b[1;32m     25\u001b[0m         loss_fn\u001b[39m=\u001b[39mcriterion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m         cb_kwargs\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdesc\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mValidating | Batches\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mposition\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mleave\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mFalse\u001b[39;00m}\n\u001b[1;32m     29\u001b[0m     )\n\u001b[1;32m     30\u001b[0m     val_ppl \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39mexp(val_loss)\n",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(encoder, decoder, optimizer, lr_scheduler, loss_fn, train_dataloader, epoch, logger, callback, cb_kwargs)\u001b[0m\n\u001b[1;32m     27\u001b[0m encoder_loss \u001b[39m=\u001b[39m loss_fn(encoder_output_flat, targets)\n\u001b[1;32m     29\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mencoder_output\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mencoder_output_flat\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m decoder_output \u001b[39m=\u001b[39m decoder(targets, encoder_output)\n\u001b[1;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     33\u001b[0m encoder_loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 40\u001b[0m, in \u001b[0;36mLandmarksSeqTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask)\u001b[0m\n\u001b[1;32m     37\u001b[0m     tgt_mask \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mTransformer\u001b[39m.\u001b[39mgenerate_square_subsequent_mask(\u001b[39mlen\u001b[39m(tgt))\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mtgt\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmemory\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mtgt_mask\u001b[39m.\u001b[39mshape\u001b[39m=}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer_decoder(tgt, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask, memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[1;32m     41\u001b[0m                                   tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[1;32m     42\u001b[0m                                   memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask)\n\u001b[1;32m     43\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear(output)\n\u001b[1;32m     44\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:460\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    457\u001b[0m tgt_is_causal \u001b[39m=\u001b[39m _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[1;32m    459\u001b[0m \u001b[39mfor\u001b[39;00m mod \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers:\n\u001b[0;32m--> 460\u001b[0m     output \u001b[39m=\u001b[39m mod(output, memory, tgt_mask\u001b[39m=\u001b[39;49mtgt_mask,\n\u001b[1;32m    461\u001b[0m                  memory_mask\u001b[39m=\u001b[39;49mmemory_mask,\n\u001b[1;32m    462\u001b[0m                  tgt_key_padding_mask\u001b[39m=\u001b[39;49mtgt_key_padding_mask,\n\u001b[1;32m    463\u001b[0m                  memory_key_padding_mask\u001b[39m=\u001b[39;49mmemory_key_padding_mask,\n\u001b[1;32m    464\u001b[0m                  tgt_is_causal\u001b[39m=\u001b[39;49mtgt_is_causal,\n\u001b[1;32m    465\u001b[0m                  memory_is_causal\u001b[39m=\u001b[39;49mmemory_is_causal)\n\u001b[1;32m    467\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    468\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:847\u001b[0m, in \u001b[0;36mTransformerDecoderLayer.forward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    846\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal))\n\u001b[0;32m--> 847\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm2(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mha_block(x, memory, memory_mask, memory_key_padding_mask, memory_is_causal))\n\u001b[1;32m    848\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm3(x \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ff_block(x))\n\u001b[1;32m    850\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/transformer.py:865\u001b[0m, in \u001b[0;36mTransformerDecoderLayer._mha_block\u001b[0;34m(self, x, mem, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_mha_block\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor, mem: Tensor,\n\u001b[1;32m    864\u001b[0m                attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 865\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmultihead_attn(x, mem, mem,\n\u001b[1;32m    866\u001b[0m                             attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m    867\u001b[0m                             key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m    868\u001b[0m                             is_causal\u001b[39m=\u001b[39;49mis_causal,\n\u001b[1;32m    869\u001b[0m                             need_weights\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    870\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout2(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/activation.py:1241\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1228\u001b[0m         query, key, value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_dim, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads,\n\u001b[1;32m   1229\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_weight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m         average_attn_weights\u001b[39m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1239\u001b[0m         is_causal\u001b[39m=\u001b[39mis_causal)\n\u001b[1;32m   1240\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1241\u001b[0m     attn_output, attn_output_weights \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mmulti_head_attention_forward(\n\u001b[1;32m   1242\u001b[0m         query, key, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed_dim, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads,\n\u001b[1;32m   1243\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_weight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49min_proj_bias,\n\u001b[1;32m   1244\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_k, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias_v, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madd_zero_attn,\n\u001b[1;32m   1245\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mout_proj\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m   1246\u001b[0m         training\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining,\n\u001b[1;32m   1247\u001b[0m         key_padding_mask\u001b[39m=\u001b[39;49mkey_padding_mask,\n\u001b[1;32m   1248\u001b[0m         need_weights\u001b[39m=\u001b[39;49mneed_weights,\n\u001b[1;32m   1249\u001b[0m         attn_mask\u001b[39m=\u001b[39;49mattn_mask,\n\u001b[1;32m   1250\u001b[0m         average_attn_weights\u001b[39m=\u001b[39;49maverage_attn_weights,\n\u001b[1;32m   1251\u001b[0m         is_causal\u001b[39m=\u001b[39;49mis_causal)\n\u001b[1;32m   1252\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_first \u001b[39mand\u001b[39;00m is_batched:\n\u001b[1;32m   1253\u001b[0m     \u001b[39mreturn\u001b[39;00m attn_output\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:5300\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m use_separate_proj_weight:\n\u001b[1;32m   5299\u001b[0m     \u001b[39massert\u001b[39;00m in_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is False but in_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 5300\u001b[0m     q, k, v \u001b[39m=\u001b[39m _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n\u001b[1;32m   5301\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5302\u001b[0m     \u001b[39massert\u001b[39;00m q_proj_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39muse_separate_proj_weight is True but q_proj_weight is None\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/functional.py:4836\u001b[0m, in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4834\u001b[0m     b_q, b_kv \u001b[39m=\u001b[39m b\u001b[39m.\u001b[39msplit([E, E \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m])\n\u001b[1;32m   4835\u001b[0m q_proj \u001b[39m=\u001b[39m linear(q, w_q, b_q)\n\u001b[0;32m-> 4836\u001b[0m kv_proj \u001b[39m=\u001b[39m linear(k, w_kv, b_kv)\n\u001b[1;32m   4837\u001b[0m \u001b[39m# reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[39;00m\n\u001b[1;32m   4838\u001b[0m kv_proj \u001b[39m=\u001b[39m kv_proj\u001b[39m.\u001b[39munflatten(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, (\u001b[39m2\u001b[39m, E))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mcontiguous()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1188x50000 and 200x400)"
     ]
    }
   ],
   "source": [
    "if \"cpu\" in str(DEVICE):\n",
    "    q = input(\"Warning: you are training on CPU. It will be slow. Press [Enter] to continue... [q] to leave\")\n",
    "    if \"q\" in q.lower():\n",
    "        raise KeyboardInterrupt(\"User aborted training\")\n",
    "\n",
    "\n",
    "metrics_history = []\n",
    "best_val_loss = float('inf')\n",
    "for epoch in tqdm(range(1, EPOCHS + 1), desc=\"Epochs\"):\n",
    "    epoch_start_time = time.time()\n",
    "    train_epoch(\n",
    "        encoder=encoder,\n",
    "        decoder=decoder,\n",
    "        optimizer=optimizer,\n",
    "        lr_scheduler=scheduler,\n",
    "        loss_fn=criterion,\n",
    "        train_dataloader=train_dl,\n",
    "        epoch=epoch,\n",
    "        logger=logger,\n",
    "        callback=tqdm,\n",
    "        cb_kwargs={\"desc\": \"Training | Batches\", \"position\": 0, \"leave\": True}\n",
    "    )\n",
    "    val_loss = evaluate(\n",
    "        model=encoder,\n",
    "        loss_fn=criterion,\n",
    "        eval_dataloader=val_dl,\n",
    "        callback=tqdm,\n",
    "        cb_kwargs={\"desc\": \"Validating | Batches\", \"position\": 1, \"leave\": False}\n",
    "    )\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger.info(f'  end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:10.6f} | valid ppl {val_ppl:8.2f}')\n",
    "\n",
    "    metrics_history.append({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        # \"TrainLoss\": train_loss,\n",
    "        \"ValLoss\": val_loss,\n",
    "        \"ValPpl\": val_ppl,\n",
    "        \"EpochTrainingTime\": elapsed,\n",
    "    })\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(encoder.state_dict(), best_encoder_params_path)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "logger.success(f\"Done training. Best model parameters saved to {best_encoder_params_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_records(metrics_history)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.ValLoss.plot()\n",
    "plt.title(f\"Validation loss ({SAMPLE_FRAC*100:.1f}% of dataset)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
