{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pose Landmarks Sequence (Sign Language) to Polish Tranlation\n",
    "> Architecture: **Encoder-Decoder** with **Attention** Mechanism, [**Transformer**](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import time\n",
    "from typing import Dict, List, Any, Callable\n",
    "from jsonlines import jsonlines\n",
    "from tqdm.notebook import tqdm\n",
    "from loguru import logger\n",
    "\n",
    "from src.settings import PREPROCESSED_DIR, MODELS_DIR\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, records: List[Dict[str, Any]]):\n",
    "        self.records = records\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        out_polish_token_ids = torch.tensor(self.records[index][\"PolishAnnotationTokenIds\"], dtype=torch.int32)\n",
    "        if len(out_polish_token_ids) > 66:\n",
    "            # FIXME: this is a hack to make it work with the model; only one clip is affected\n",
    "            print(\"Warning: PolishAnnotationTokenIds is longer than 66\")\n",
    "            out_polish_token_ids = out_polish_token_ids[:66]\n",
    "\n",
    "        # (seq_len x 33 x 3) -> (seq_len x 99)\n",
    "        frame_seq_landmarks = torch.tensor(self.records[index][\"FramesLandmarksCoords\"], dtype=torch.float32).view(-1, 99)\n",
    "        prepro_landmarks_seq = self.preprocess_landmarks_seq(frame_seq_landmarks)\n",
    "\n",
    "        return {\n",
    "            \"in_landmarks\": prepro_landmarks_seq,\n",
    "            \"out_polish_token_ids\": out_polish_token_ids,\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_landmarks_seq(landmarks_seq, long_multiplier=10_000):\n",
    "        x = landmarks_seq * long_multiplier\n",
    "        x = landmarks_seq + torch.min(landmarks_seq) * -1\n",
    "        return x.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m SAMPLE_FRAC \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m jsonlines\u001b[39m.\u001b[39mopen(PREPROCESSED_DIR \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclips_dataset_wth_herbert_token_ids.jsonl\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m reader:\n\u001b[1;32m      5\u001b[0m     \u001b[39m# total_records: 19_503\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     raw_records \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m((rec \u001b[39mfor\u001b[39;49;00m rec \u001b[39min\u001b[39;49;00m reader \u001b[39mif\u001b[39;49;00m np\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice([\u001b[39mTrue\u001b[39;49;00m, \u001b[39mFalse\u001b[39;49;00m], p\u001b[39m=\u001b[39;49m[SAMPLE_FRAC, \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m SAMPLE_FRAC])))\n\u001b[1;32m      8\u001b[0m     \u001b[39m# raw_records = list(reader)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_records, val_records \u001b[39m=\u001b[39m train_test_split(raw_records, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m SAMPLE_FRAC \u001b[39m=\u001b[39m \u001b[39m0.2\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m jsonlines\u001b[39m.\u001b[39mopen(PREPROCESSED_DIR \u001b[39m/\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mclips_dataset_wth_herbert_token_ids.jsonl\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m reader:\n\u001b[1;32m      5\u001b[0m     \u001b[39m# total_records: 19_503\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     raw_records \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m((rec \u001b[39mfor\u001b[39;00m rec \u001b[39min\u001b[39;00m reader \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice([\u001b[39mTrue\u001b[39;00m, \u001b[39mFalse\u001b[39;00m], p\u001b[39m=\u001b[39m[SAMPLE_FRAC, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m SAMPLE_FRAC])))\n\u001b[1;32m      8\u001b[0m     \u001b[39m# raw_records = list(reader)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m train_records, val_records \u001b[39m=\u001b[39m train_test_split(raw_records, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jsonlines/jsonlines.py:434\u001b[0m, in \u001b[0;36mReader.iter\u001b[0;34m(self, type, allow_none, skip_empty, skip_invalid)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m         \u001b[39myield\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    435\u001b[0m             \u001b[39mtype\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39mtype\u001b[39;49m, allow_none\u001b[39m=\u001b[39;49mallow_none, skip_empty\u001b[39m=\u001b[39;49mskip_empty\n\u001b[1;32m    436\u001b[0m         )\n\u001b[1;32m    437\u001b[0m     \u001b[39mexcept\u001b[39;00m InvalidLineError:\n\u001b[1;32m    438\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m skip_invalid:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/jsonlines/jsonlines.py:307\u001b[0m, in \u001b[0;36mReader.read\u001b[0;34m(self, type, allow_none, skip_empty)\u001b[0m\n\u001b[1;32m    304\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39minvalid type specified\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    306\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     lineno, line \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_line_iter)\n\u001b[1;32m    308\u001b[0m     \u001b[39mwhile\u001b[39;00m skip_empty \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m line\u001b[39m.\u001b[39mrstrip():\n\u001b[1;32m    309\u001b[0m         lineno, line \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_line_iter)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/codecs.py:319\u001b[0m, in \u001b[0;36mBufferedIncrementalDecoder.decode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_buffer_decode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, errors, final):\n\u001b[1;32m    315\u001b[0m     \u001b[39m# Overwrite this method in subclasses: It must decode input\u001b[39;00m\n\u001b[1;32m    316\u001b[0m     \u001b[39m# and return an (output, length consumed) tuple\u001b[39;00m\n\u001b[1;32m    317\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, final\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    320\u001b[0m     \u001b[39m# decode input (taking the buffer into account)\u001b[39;00m\n\u001b[1;32m    321\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer \u001b[39m+\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    322\u001b[0m     (result, consumed) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_buffer_decode(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merrors, final)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "SAMPLE_FRAC = 0.2\n",
    "\n",
    "\n",
    "with jsonlines.open(PREPROCESSED_DIR / \"clips_dataset_wth_herbert_token_ids.jsonl\") as reader:\n",
    "    # total_records: 19_503\n",
    "\n",
    "    raw_records = list((rec for rec in reader if np.random.choice([True, False], p=[SAMPLE_FRAC, 1 - SAMPLE_FRAC])))\n",
    "    # raw_records = list(reader)\n",
    "\n",
    "train_records, val_records = train_test_split(raw_records, test_size=0.2, random_state=42)\n",
    "\n",
    "train_ds, val_ds = ClipsDataset(train_records), ClipsDataset(val_records)\n",
    "\n",
    "del train_records, val_records, raw_records\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clips_df = pd.DataFrame.from_records(raw_records)\n",
    "# clips_df.FramesLandmarksCoords.apply(lambda x: np.array(x).flatten()).explode().astype(\"float\").describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([record[\"NumFrames\"] for record in raw_records]).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.Series([record[\"PolishAnnotationTokenIds\"] for record in raw_records]).map(len).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for record in train_ds:\n",
    "    print(record[\"in_landmarks\"].shape)  # n_frames x n_landmarks*3\n",
    "    print(record[\"out_polish_token_ids\"].shape)  # padded n_tokens\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_LANDMARKS = 33\n",
    "COORD_CHANNELS = 3\n",
    "MAX_TOKENS = 66\n",
    "MAX_FRAMES = 392"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandmarksSeqTransformerEncoder(nn.Module):\n",
    "    # https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "\n",
    "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
    "                 nlayers: int, dropout: float = 0.5, device: torch.device = torch.device(\"cuda:0\")) -> None:\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, d_hid, dropout, batch_first=False)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.embedding = nn.Embedding(ntoken, d_model)\n",
    "        self.d_model = d_model\n",
    "        self.linear = nn.Linear(d_model, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self) -> None:\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.linear.bias.data.zero_()\n",
    "        self.linear.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src: Tensor, src_mask: Tensor = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            src: Tensor, shape ``[seq_len, batch_size]`` , 99\n",
    "            src_mask: Tensor, shape ``[seq_len, seq_len]``\n",
    "\n",
    "        Returns:\n",
    "            output Tensor of shape ``[seq_len, batch_size, ntoken]``\n",
    "        \"\"\"\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "\n",
    "        src = self.pos_encoder(src)  # batch_len, 99, batch_size, d_model\n",
    "        \n",
    "        _, _, batch_size, d_model = src.shape\n",
    "        src = src.view(-1, batch_size, d_model)\n",
    "\n",
    "        if src_mask is None:\n",
    "            \"\"\"Generate a square causal mask for the sequence. The masked positions are filled with float('-inf').\n",
    "            Unmasked positions are filled with float(0.0).\n",
    "            \"\"\"\n",
    "            src_mask = nn.Transformer.generate_square_subsequent_mask(len(src)).to(self.device)\n",
    "        # print(f\"{src.shape=}\\n{src_mask.shape=}\")\n",
    "        output = self.transformer_encoder(src, src_mask)\n",
    "        output = self.linear(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    # https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/#:~:text=What%20Is%20Positional%20Encoding%3F\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)  # max_len x 1\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))  # d_model/2\n",
    "        pe = torch.zeros(max_len, 1, 1, d_model)  # max_len x 99 x 1 x d_model\n",
    "        # print(f\"{position.shape=}\\n{div_term.shape=}\\n{pe.shape=}\")\n",
    "        pe[:, 0, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``\n",
    "        \"\"\"\n",
    "        # x : batch_len, 99, batch_size, d_model\n",
    "        # print(x.shape, x)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(dataset, batch_size: int, device: torch.device) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "\n",
    "    # to work wth dataset:\n",
    "    clips_landmarks_seq = torch.cat([record[\"in_landmarks\"] for record in dataset])\n",
    "    \n",
    "    # oryginalnie data: (20857)\n",
    "\n",
    "    # clips_landmarks_seq = [record[\"in_landmarks\"] for record in data]\n",
    "    # print(clips_landmarks_seq.shape)\n",
    "    data = torch.tensor(clips_landmarks_seq)\n",
    "\n",
    "    batch_len = data.size(0) // batch_size\n",
    "    # print(seq_len)\n",
    "    # data = data.view(batch_size, seq_len)\n",
    "    data = data[:batch_len * batch_size , :]  # batch_len x 99\n",
    "    # print(data.shape)\n",
    "\n",
    "    data = data.view(batch_len, -1, batch_size)\n",
    "    # data = data.t()\n",
    "    # print(data.shape, data)\n",
    "    data = data.contiguous()  # batch_len x 99 x batch_size\n",
    "    # print(data.shape, data)\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "def get_batch(batched_data_dl: Tensor, i: int, bptt: int = 35):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "\n",
    "    query_batch_len = min(bptt, len(batched_data_dl) - 1 - i)\n",
    "    data = batched_data_dl[i:i+query_batch_len, :, :]\n",
    "    target = batched_data_dl[i+1:i+1+query_batch_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BPTT = 35\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "train_dl = batchify(train_ds, batch_size=BATCH_SIZE, device=DEVICE)\n",
    "val_dl = batchify(val_ds, batch_size=BATCH_SIZE, device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_ds[0][\"in_landmarks\"].shape)\n",
    "\n",
    "# data = torch.cat([record[\"in_landmarks\"] for record in train_ds])\n",
    "# print(data.shape, data, \"\\n\")\n",
    "print(f\"{train_dl.shape=}\")\n",
    "\n",
    "for batch, i in enumerate(range(0, train_dl.size(0) - 1, BPTT)):\n",
    "    if batch == 1:\n",
    "        break\n",
    "    # print(input_.shape)\n",
    "    data, targets = get_batch(train_dl, i, bptt=BPTT)\n",
    "    print(f\"{data.shape=}\")\n",
    "    print(f\"{targets.shape=}\")\n",
    "\n",
    "del train_ds, val_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50_000  # aka lenght of vector that contains probabilities for each token; must match the tokenizer `token_ids` range (0 included)\n",
    "DROPOUT = 0.2  # dropout probability\n",
    "EMBEDDING_LAYER_OUT_SIZE = 200  # embedding dimension\n",
    "\n",
    "ENCODING_TRANSFORMER_LAYERS = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "ENCODING_TRANSFORMER_HIDDEN_DIM = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "ENCODING_TRANSFORMER_NHEAD = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "\n",
    "\n",
    "encoder = LandmarksSeqTransformerEncoder(\n",
    "    ntoken=VOCAB_SIZE,\n",
    "    d_model=EMBEDDING_LAYER_OUT_SIZE,\n",
    "    nhead=ENCODING_TRANSFORMER_NHEAD,\n",
    "    d_hid=ENCODING_TRANSFORMER_HIDDEN_DIM,\n",
    "    nlayers=ENCODING_TRANSFORMER_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    "    lr_scheduler: optim.lr_scheduler.LRScheduler,\n",
    "    loss_fn: nn.Module,\n",
    "    train_dataloader: Tensor,\n",
    "    epoch: int,\n",
    "    callback: Callable,\n",
    "    cb_kwargs: dict\n",
    ") -> None:\n",
    "    \n",
    "    global BPTT, VOCAB_SIZE\n",
    "\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 200\n",
    "    start_time = time.time()\n",
    "    num_batches = len(train_dataloader) // BPTT\n",
    "    for batch, step in callback(enumerate(range(0, train_dataloader.size(0) - 1, BPTT)), total=num_batches, **cb_kwargs):\n",
    "        data, targets = get_batch(train_dataloader, step)\n",
    "        output = model(data)\n",
    "        output_flat = output.view(-1, VOCAB_SIZE)\n",
    "        loss = loss_fn(output_flat, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            lr = lr_scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            logger.info(f'  epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: nn.Module,\n",
    "    loss_fn: nn.Module,\n",
    "    eval_dataloader: Tensor,\n",
    "    callback: Callable,\n",
    "    cb_kwargs: dict\n",
    ") -> float:\n",
    "\n",
    "    global BPTT, VOCAB_SIZE\n",
    "\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    dl_len = len(eval_dataloader)\n",
    "    num_batches = dl_len // BPTT\n",
    "    total_loss = 0.\n",
    "    with torch.no_grad():\n",
    "        for i in callback(range(0, eval_dataloader.size(0) - 1, BPTT), total=num_batches, **cb_kwargs):\n",
    "            data, targets = get_batch(eval_dataloader, i)\n",
    "            seq_len = data.size(0)\n",
    "            output = model(data)\n",
    "            output_flat = output.view(-1, VOCAB_SIZE)\n",
    "            total_loss += seq_len * loss_fn(output_flat, targets).item()\n",
    "    return total_loss / (dl_len - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_safe_model_name(model: nn.Module, params: Dict[str, int | float]) -> str:\n",
    "    params_safe_str = \"__\".join([\n",
    "        f\"{key}{f'{v:3f}'.replace('.', 'f') if isinstance(v, float) else v}\"\n",
    "        for key, v in params.items()\n",
    "    ])\n",
    "    return f\"{model.__class__.__name__}__{params_safe_str}.best.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LEARNING_RATE = 5\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "encoder_scheduler = optim.lr_scheduler.StepLR(encoder_optimizer, step_size=1.0, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_safe_model_name(encoder, params={\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"vocab\": VOCAB_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"cpu\" in str(DEVICE):\n",
    "    q = input(\"Warning: you are training on CPU. It will be slow. Press [Enter] to continue... [q] to leave\")\n",
    "    if \"q\" in q.lower():\n",
    "        raise KeyboardInterrupt(\"User aborted training\")\n",
    "\n",
    "\n",
    "safe_model_name = get_safe_model_name(encoder, params={\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"vocab\": VOCAB_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "})\n",
    "\n",
    "metrics_history = []\n",
    "best_val_loss = float('inf')\n",
    "best_encoder_params_path = MODELS_DIR / safe_model_name\n",
    "for epoch in tqdm(range(1, EPOCHS + 1)):\n",
    "    epoch_start_time = time.time()\n",
    "    train_epoch(\n",
    "        model=encoder,\n",
    "        optimizer=encoder_optimizer,\n",
    "        lr_scheduler=encoder_scheduler,\n",
    "        loss_fn=criterion,\n",
    "        train_dataloader=train_dl,\n",
    "        epoch=epoch,\n",
    "        callback=tqdm,\n",
    "        cb_kwargs={\"desc\": \"Training | Batches\", \"position\": 0, \"leave\": True}\n",
    "    )\n",
    "    val_loss = evaluate(\n",
    "        model=encoder,\n",
    "        loss_fn=criterion,\n",
    "        eval_dataloader=val_dl,\n",
    "        callback=tqdm,\n",
    "        cb_kwargs={\"desc\": \"Validating | Batches\", \"position\": 1, \"leave\": False}\n",
    "    )\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    logger(f'  end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "        f'valid loss {val_loss:10.6f} | valid ppl {val_ppl:8.2f}')\n",
    "\n",
    "    metrics_history.append({\n",
    "        \"Epoch\": epoch + 1,\n",
    "        # \"TrainLoss\": train_loss,\n",
    "        \"ValLoss\": val_loss,\n",
    "        \"ValPpl\": val_ppl,\n",
    "        \"EpochTrainingTime\": elapsed,\n",
    "    })\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(encoder.state_dict(), best_encoder_params_path)\n",
    "\n",
    "    encoder_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = pd.DataFrame.from_records(metrics_history)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df.ValLoss.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bardzo Stary LSTM Nic Nie Warty :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LandmarksTranslator(nn.Module):\n",
    "    \n",
    "#     def __init__(self, input_dim: int, output_dim: int):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#         # self.lstm = nn.LSTM(input_dim, output_dim, batch_first=True)\n",
    "#         self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         # x = x.view(-1, self.input_dim)\n",
    "\n",
    "#         output, (hn, cn) = self.lstm(x)\n",
    "#         # print(output.shape)\n",
    "#         x = output\n",
    "\n",
    "#         # print(x)\n",
    "#         # print(output.shape, hn.shape, cn.shape)\n",
    "#         # x = self.linear(x)\n",
    "\n",
    "#         # try get `token_ids` imitation\n",
    "#         min_id, max_id = 0, 25_000\n",
    "#         min_x_val = x.min(axis=0).values\n",
    "#         max_x_val = x.max(axis=0).values\n",
    "#         # print(min_x_val, max_x_val)\n",
    "        \n",
    "#         x = (x - min_x_val) / (max_x_val - min_x_val)  # normalize to [0, 1]\n",
    "#         x = (x  * (max_id - min_id)) + min_id  # scale to [min_id, max_id]\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LandmarksTranslatorT5(nn.Module):\n",
    "\n",
    "#     def __init__(self, input_dim: int, output_dim: int):\n",
    "#         super().__init__()\n",
    "#         self.input_dim = input_dim\n",
    "#         self.output_dim = output_dim\n",
    "\n",
    "#         self.t5 = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         print(x.shape)\n",
    "#         # x: (batch_size x seq_len x vec_size)\n",
    "#         x = self.t5.forward(inputs_embeds=x, output_hidden_states=True)\n",
    "\n",
    "#         # # try get `token_ids` imitation\n",
    "#         # min_id, max_id = 0, 25_000\n",
    "#         # min_x_val = x.min(axis=0).values\n",
    "#         # max_x_val = x.max(axis=0).values\n",
    "#         # # print(min_x_val, max_x_val)\n",
    "        \n",
    "#         # x = (x - min_x_val) / (max_x_val - min_x_val)  # normalize to [0, 1]\n",
    "#         # x = (x  * (max_id - min_id)) + min_id  # scale to [min_id, max_id]\n",
    "\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train(model, train_dl, val_dl, optimizer, loss_fn, epochs, device=\"cuda\"):\n",
    "#     herbert_tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")\n",
    "#     model_name = model.__class__.__name__\n",
    "#     metrics_history = []\n",
    "#     for epoch in tqdm(range(epochs), desc=\"Epochs: \", leave=True, position=0):\n",
    "#         last_epoch = epoch == epochs - 1\n",
    "#         train_losses = []\n",
    "#         val_losses = []\n",
    "#         val_accs = []\n",
    "#         # for n_batch, batch in enumerate(tqdm(train_dl, desc=\"Batches: \", leave=False, position=1)):\n",
    "#         for n_batch, batch in enumerate(train_dl):\n",
    "#             X_batch = batch[\"in_landmarks\"].to(device)\n",
    "#             y_batch = batch[\"out_polish_token_ids\"].to(device)\n",
    "#             # print(y_batch.shape)\n",
    "\n",
    "#             y_pred = model(X_batch)\n",
    "#             # print(y_pred.shape)\n",
    "\n",
    "#             first_batch = n_batch == 0\n",
    "#             if first_batch and (epoch % ceil(epochs * 0.1) == 0 or last_epoch):\n",
    "#                 decoded_y_batch = herbert_tokenizer.decode(y_batch[0], skip_special_tokens=True)\n",
    "#                 decoded_y_pred_batch = herbert_tokenizer.decode(y_pred[0].int(), skip_special_tokens=True)\n",
    "#                 print(f\"{decoded_y_batch=}\")\n",
    "#                 print(f\"{decoded_y_pred_batch=}\")\n",
    "#                 # for decoded_y, decoded_y_pred in zip(decoded_y_batch, decoded_y_pred_batch):\n",
    "#                 #     print(f\"{decoded_y=}\")\n",
    "#                 #     print(f\"{decoded_y_pred=}\")\n",
    "#                 #     print()\n",
    "#                 print(\"-\".center(80, \"-\"))\n",
    "\n",
    "#             if y_pred.shape != y_batch.shape:\n",
    "#                 print(f\"{y_pred.shape=}, {y_batch.shape=} mismatched!\")\n",
    "#                 continue\n",
    "                \n",
    "#             loss = loss_fn(y_pred, y_batch.float())\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             loss.backward()  # backward pass, calculate gradients\n",
    "#             optimizer.step()  # update weights\n",
    "\n",
    "#             # val_loss, val_acc = validate(model, val_dl, loss_fn, device=device)\n",
    "#             val_loss = validate(model, val_dl, loss_fn, device=device)\n",
    "            \n",
    "#             train_losses.append(loss.item())\n",
    "#             val_losses.append(val_loss)\n",
    "#             # val_accs.append(val_acc)\n",
    "\n",
    "#         epoch_metrics = {\n",
    "#             \"Epoch\": epoch + 1,\n",
    "#             \"ModelName\": model_name,\n",
    "#             \"TrainLoss\": np.mean(train_losses),\n",
    "#             \"ValLoss\": np.mean(val_losses),\n",
    "#             # \"ValAcc\": np.mean(val_accs)\n",
    "#         }\n",
    "#         metrics_history.append(epoch_metrics)\n",
    "\n",
    "#         if epoch % ceil(epochs * 0.1) == 0 or last_epoch:\n",
    "#             print(\n",
    "#                 f\"Epoch: {epoch + 1:<2} | \"\n",
    "#                 + \" \".join([f\"{k}: {v:.6f}\" for k, v in list(epoch_metrics.items())[2:]])\n",
    "#             )\n",
    "\n",
    "#     return metrics_history\n",
    "\n",
    "\n",
    "# def validate(model, val_dl, loss_fn, device=\"cuda\"):\n",
    "#     losses = []\n",
    "#     accuracies = []\n",
    "#     for batch in val_dl:\n",
    "#         X_batch = batch[\"in_landmarks\"].to(device)\n",
    "#         y_batch = batch[\"out_polish_token_ids\"].to(device)\n",
    "\n",
    "#         y_pred = model(X_batch)\n",
    "\n",
    "#         loss = loss_fn(y_pred, y_batch.float())\n",
    "#         losses.append(loss.item())\n",
    "        \n",
    "#         # acc = (torch.argmax(y_pred, 1) == torch.argmax(y_batch, 1)).cpu().float().mean()\n",
    "#         # accuracies.append(acc)\n",
    "\n",
    "#     # return np.mean(losses), np.mean(accuracies)\n",
    "#     return np.mean(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"Device:\", device, \"\\n\")\n",
    "\n",
    "\n",
    "# BATCH_SIZE = 4\n",
    "# EPOCHS = 100\n",
    "# LEARING_RATE = 0.001\n",
    "\n",
    "\n",
    "# assert BATCH_SIZE <= len(train_ds) and BATCH_SIZE <= len(val_ds)\n",
    "\n",
    "# train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "# val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "# model = LandmarksTranslator(\n",
    "# # model = LandmarksTranslatorT5(\n",
    "#     input_dim=N_LANDMARKS*COORD_CHANNELS,\n",
    "#     output_dim=MAX_TOKENS\n",
    "# ).to(device)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=LEARING_RATE)\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# metrics_history = train(model, train_dl, val_dl, optimizer, loss_fn, epochs=EPOCHS, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
