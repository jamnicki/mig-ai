{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import math\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from jsonlines import jsonlines\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "from src.settings import PREPROCESSED_DIR, MODELS_DIR, LOGS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# DEVICE = torch.device(\"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    # return torch.device('cpu')\n",
    "\n",
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1]\n",
    "    scaled = torch.matmul(q, k.transpose(-1, -2)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scaled = scaled.permute(1, 0, 2, 3) + mask\n",
    "        scaled = scaled.permute(1, 0, 2, 3)\n",
    "    attention = F.softmax(scaled, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_sequence_length):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self):\n",
    "        even_i = torch.arange(0, self.d_model, 2).float()\n",
    "        denominator = torch.pow(10000, even_i/self.d_model)\n",
    "        position = (torch.arange(self.max_sequence_length)\n",
    "                          .reshape(self.max_sequence_length, 1))\n",
    "        even_PE = torch.sin(position / denominator)\n",
    "        odd_PE = torch.cos(position / denominator)\n",
    "        stacked = torch.stack([even_PE, odd_PE], dim=2)\n",
    "        PE = torch.flatten(stacked, start_dim=1, end_dim=2)\n",
    "        return PE\n",
    "\n",
    "class SentenceEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_sequence_length, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_encoder = PositionalEncoding(d_model, max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x): # batched tokens ids\n",
    "        x = self.embedding(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model , 3 * d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        batch_size, sequence_length, d_model = x.size()\n",
    "        qkv = self.qkv_layer(x)\n",
    "        qkv = qkv.reshape(batch_size, sequence_length, self.num_heads, 3 * self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask)\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, self.num_heads * self.head_dim)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameters_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameters_shape=parameters_shape\n",
    "        self.eps=eps\n",
    "        self.gamma = nn.Parameter(torch.ones(parameters_shape))\n",
    "        self.beta =  nn.Parameter(torch.zeros(parameters_shape))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        dims = [-(i + 1) for i in range(len(self.parameters_shape))]\n",
    "        mean = inputs.mean(dim=dims, keepdim=True)\n",
    "        var = ((inputs - mean) ** 2).mean(dim=dims, keepdim=True)\n",
    "        std = (var + self.eps).sqrt()\n",
    "        y = (inputs - mean) / std\n",
    "        out = self.gamma * y + self.beta\n",
    "        return out\n",
    "\n",
    "  \n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attention(x, mask=self_attention_mask)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + residual_x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + residual_x)\n",
    "        return x\n",
    "    \n",
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, self_attention_mask  = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "class FrameLandmarksEmbedding(nn.Module):\n",
    "    \"For a given sentence, create an embedding\"\n",
    "    def __init__(self, max_frames, d_model, n_landmarks=99):\n",
    "        super().__init__()\n",
    "        self.max_sequence_length = max_frames\n",
    "        self.linear = nn.Linear(n_landmarks, d_model)\n",
    "        self.position_encoder = PositionalEncoding(d_model, self.max_sequence_length)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):  # seq of frames (landmarks), batched\n",
    "        # in: (batch_size, n_frames, 99)\n",
    "        # x = x[:, :self.max_sequence_length, :]\n",
    "        # x = F.pad(x, (0, 0, 0, self.max_sequence_length - x.size(1)), value=-1)\n",
    "        x = self.linear(x)\n",
    "        pos = self.position_encoder().to(get_device())\n",
    "        x = self.dropout(x + pos)\n",
    "        # out: (batch_size, max_sequence_length, d_model)\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length\n",
    "                ):\n",
    "        super().__init__()\n",
    "        # self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, language_to_index, START_TOKEN, END_TOKEN, PADDING_TOKEN)\n",
    "        self.frames_seq_embedding = FrameLandmarksEmbedding(max_sequence_length, d_model)\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(d_model, ffn_hidden, num_heads, drop_prob)\n",
    "                                      for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, self_attention_mask):\n",
    "        # x = self.sentence_embedding(x, start_token, end_token)\n",
    "        x = self.frames_seq_embedding(x)\n",
    "        x = self.layers(x, self_attention_mask)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadCrossAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.kv_layer = nn.Linear(d_model , 2 * d_model)\n",
    "        self.q_layer = nn.Linear(d_model , d_model)\n",
    "        self.linear_layer = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def forward(self, x, y, mask):\n",
    "        batch_size, sequence_length, d_model = x.size() # in practice, this is the same for both languages...so we can technically combine with normal attention\n",
    "        kv = self.kv_layer(x)\n",
    "        q = self.q_layer(y)\n",
    "        kv = kv.reshape(batch_size, sequence_length, self.num_heads, 2 * self.head_dim)\n",
    "        q = q.reshape(batch_size, sequence_length, self.num_heads, self.head_dim)\n",
    "        kv = kv.permute(0, 2, 1, 3)\n",
    "        q = q.permute(0, 2, 1, 3)\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        values, attention = scaled_dot_product(q, k, v, mask) # We don't need the mask for cross attention, removing in outer function!\n",
    "        values = values.permute(0, 2, 1, 3).reshape(batch_size, sequence_length, d_model)\n",
    "        out = self.linear_layer(values)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, num_heads, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm1 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.encoder_decoder_attention = MultiHeadCrossAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.layer_norm2 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.layer_norm3 = LayerNormalization(parameters_shape=[d_model])\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        _y = y.clone()\n",
    "        y = self.self_attention(y, mask=self_attention_mask)\n",
    "        y = self.dropout1(y)\n",
    "        y = self.layer_norm1(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.encoder_decoder_attention(x, y, mask=cross_attention_mask)\n",
    "        y = self.dropout2(y)\n",
    "        y = self.layer_norm2(y + _y)\n",
    "\n",
    "        _y = y.clone()\n",
    "        y = self.ffn(y)\n",
    "        y = self.dropout3(y)\n",
    "        y = self.layer_norm3(y + _y)\n",
    "        return y\n",
    "\n",
    "\n",
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_attention_mask, cross_attention_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            y = module(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d_model, \n",
    "                 ffn_hidden, \n",
    "                 num_heads, \n",
    "                 drop_prob, \n",
    "                 num_layers,\n",
    "                 max_sequence_length,\n",
    "                 vocab_size,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.sentence_embedding = SentenceEmbedding(max_sequence_length, d_model, vocab_size)\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(d_model, ffn_hidden, num_heads, drop_prob) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x, y, self_attention_mask, cross_attention_mask):\n",
    "        y = self.sentence_embedding(y)\n",
    "        y = self.layers(x, y, self_attention_mask, cross_attention_mask)\n",
    "        return y\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, \n",
    "                d_model, \n",
    "                ffn_hidden, \n",
    "                num_heads, \n",
    "                drop_prob, \n",
    "                num_layers,\n",
    "                max_sequence_length, \n",
    "                vocab_size,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length)\n",
    "        self.decoder = Decoder(d_model, ffn_hidden, num_heads, drop_prob, num_layers, max_sequence_length, vocab_size)\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    def forward(self, \n",
    "                x, \n",
    "                y, \n",
    "                encoder_self_attention_mask=None, \n",
    "                decoder_self_attention_mask=None, \n",
    "                decoder_cross_attention_mask=None,\n",
    "    ):\n",
    "        x = self.encoder(x, encoder_self_attention_mask)\n",
    "        out = self.decoder(x, y, decoder_self_attention_mask, decoder_cross_attention_mask)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_HEADS = 8\n",
    "DROP_PROB = 0.1\n",
    "NUM_LAYERS = 5\n",
    "D_MODEL = 512\n",
    "FFN_HIDDEN = 2048\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 50  # max in frames = max out tokens\n",
    "N_LANDMARKS = 99\n",
    "VOCAB_SIZE = 50_000\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "SAMPLE_FRAC = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipsDataset(Dataset):\n",
    "\n",
    "    def __init__(self, records: List[Dict[str, Any]], max_input_lenght, max_output_length):\n",
    "        self.records = records\n",
    "        self.max_input_lenght = max_input_lenght\n",
    "        self.max_output_length = max_output_length\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.records)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, torch.Tensor]:\n",
    "        out_polish_token_ids = torch.tensor(self.records[index][\"PolishAnnotationTokenIds\"], dtype=torch.int32)\n",
    "        if len(out_polish_token_ids) > 66:\n",
    "            # FIXME: this is a hack to make it work with the model; only one clip is affected\n",
    "            print(\"Warning: PolishAnnotationTokenIds is longer than 66\")\n",
    "            out_polish_token_ids = out_polish_token_ids[:66]\n",
    "\n",
    "        out_polish_token_ids = out_polish_token_ids[:self.max_output_length]\n",
    "\n",
    "        # (seq_len x 33 x 3) -> (seq_len x 99)\n",
    "        frame_seq_landmarks = torch.tensor(self.records[index][\"FramesLandmarksCoords\"], dtype=torch.float32).view(-1, 99)\n",
    "        every_nth_frame_seq_landmarks = frame_seq_landmarks[::4]  # get every 4th frame\n",
    "        seq_pad_len = self.max_input_lenght - every_nth_frame_seq_landmarks.size(0)\n",
    "        padded_frame_seq_landmarks = F.pad(every_nth_frame_seq_landmarks, (0, 0, 0, seq_pad_len), value=0)  # 200x99\n",
    "        # prepro_landmarks_seq = self.preprocess_landmarks_seq(frame_seq_landmarks)\n",
    "\n",
    "        return {\n",
    "            \"in_landmarks\": padded_frame_seq_landmarks,\n",
    "            \"out_polish_token_ids\": out_polish_token_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open(PREPROCESSED_DIR / \"clips_dataset_wth_herbert_token_ids.jsonl\") as reader:\n",
    "    # total_records: 19_503\n",
    "    if SAMPLE_FRAC < 1:\n",
    "        raw_records = list((rec for rec in reader if np.random.choice([True, False], p=[SAMPLE_FRAC, 1 - SAMPLE_FRAC])))  # iterable approach for random sample\n",
    "    else:\n",
    "        raw_records = list(reader)\n",
    "\n",
    "train_records, val_records = train_test_split(raw_records, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7813, 1954)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = ClipsDataset(train_records, max_input_lenght=MAX_SEQUENCE_LENGTH, max_output_length=MAX_SEQUENCE_LENGTH)\n",
    "val_ds = ClipsDataset(val_records, max_input_lenght=MAX_SEQUENCE_LENGTH, max_output_length=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# del train_records, val_records, raw_records\n",
    "\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 99])\n",
      "torch.Size([50])\n"
     ]
    }
   ],
   "source": [
    "for record in train_ds:\n",
    "    print(record[\"in_landmarks\"].shape)  # n_frames x n_landmarks*3\n",
    "    print(record[\"out_polish_token_ids\"].shape)  # padded n_tokens\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, pin_memory=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 50, 99])\n",
      "torch.Size([32, 50])\n"
     ]
    }
   ],
   "source": [
    "for record in train_dl:\n",
    "    print(record[\"in_landmarks\"].shape)  # n_frames x n_landmarks*3\n",
    "    print(record[\"out_polish_token_ids\"].shape)  # padded n_tokens\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-4\n",
    "TOKEN_PROB_THRESHOLD = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    d_model=D_MODEL,\n",
    "    ffn_hidden=FFN_HIDDEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    drop_prob=DROP_PROB,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"allegro/herbert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer.pad_token_id=1\n"
     ]
    }
   ],
   "source": [
    "print(f\"{tokenizer.pad_token_id=}\")\n",
    "criterion = nn.CrossEntropyLoss(\n",
    "    ignore_index=tokenizer.pad_token_id,\n",
    "    reduction=\"none\"\n",
    ")\n",
    "\n",
    "# # When computing the loss, we are ignoring cases when the label is the padding token\n",
    "# for params in transformer.parameters():\n",
    "#     if params.dim() > 1:\n",
    "#         nn.init.xavier_uniform_(params)\n",
    "\n",
    "optim = torch.optim.Adam(transformer.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(seq_landmarks_batch, token_ids_batch):\n",
    "    num_sentences = len(seq_landmarks_batch)\n",
    "    look_ahead_mask = torch.full([MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "      seq_landmarks_len, seq_token_ids_len = len(seq_landmarks_batch[idx]), len(token_ids_batch[idx])\n",
    "      seq_landmarks_pad_mask_ids = np.arange(seq_landmarks_len + 1, MAX_SEQUENCE_LENGTH)\n",
    "      seq_token_ids_pad_mask_ids = np.arange(seq_token_ids_len + 1, MAX_SEQUENCE_LENGTH)\n",
    "      encoder_padding_mask[idx, :, seq_landmarks_pad_mask_ids] = True\n",
    "      encoder_padding_mask[idx, seq_landmarks_pad_mask_ids, :] = True\n",
    "      decoder_padding_mask_self_attention[idx, :, seq_token_ids_pad_mask_ids] = True\n",
    "      decoder_padding_mask_self_attention[idx, seq_token_ids_pad_mask_ids, :] = True\n",
    "      decoder_padding_mask_cross_attention[idx, :, seq_landmarks_pad_mask_ids] = True\n",
    "      decoder_padding_mask_cross_attention[idx, seq_token_ids_pad_mask_ids, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_prediction_batch(pred_batch, tokenizer, threshold=0.3):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    50000.000000\n",
      "mean         0.000020\n",
      "std          0.000012\n",
      "min          0.000002\n",
      "25%          0.000011\n",
      "50%          0.000017\n",
      "75%          0.000025\n",
      "max          0.000177\n",
      "dtype: float64\n",
      "Iteration 0 : 10.904820442199707\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: duchowe tolerancji pierwoSowizorganizowanie bynajmniej odwiedzin Ośrodkiem przyniosła tendencje ającekspozycji skakskakrezygnują duchowe tendencje Ministrów macieMinistrów Ministrów uciekają Ministrów macienapastJabłonzainteresowała napastaga rezygnują napastnapastnapastŻołnierzy napastdniami dniami dniami uciekają postawił napastdniami Тdniami zabiegał zadownapastMinistrów Ministrów koledzy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     1.999999e-05\n",
      "std      4.360072e-03\n",
      "min      3.986099e-08\n",
      "25%      2.699204e-07\n",
      "50%      4.047473e-07\n",
      "75%      6.122288e-07\n",
      "max      9.749520e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 3.6414968967437744\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant to się. się się się się się się się się się Jak się się się się się się się się się się się się z się się się się\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.428760e-03\n",
      "min      1.066946e-08\n",
      "25%      8.542509e-08\n",
      "50%      1.338538e-07\n",
      "75%      2.137226e-07\n",
      "max      9.903109e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 2.8657279014587402\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie i, grui Kot. ósmej grusię uważać się znak Miuje kota uje uważać Migruuje MiJak się się ucieka się się się się się uje się się się się\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.433808e-03\n",
      "min      8.429868e-09\n",
      "25%      7.034219e-08\n",
      "50%      1.108589e-07\n",
      "75%      1.803438e-07\n",
      "max      9.914398e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 2.086927652359009\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno można wody do kąpieli z n. kota się odkota uje ka Jak MiJak domu się się Jak się Jak Jak Jak Jak Jak z z Jak Jak Jak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000004e-05\n",
      "std      4.454079e-03\n",
      "min      2.419474e-09\n",
      "25%      2.360693e-08\n",
      "50%      3.820811e-08\n",
      "75%      6.578098e-08\n",
      "max      9.959723e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 1.7459313869476318\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. uje się uje Jak się Jak Jak Jak Jak się się uje się Jak Mimogę Jak uje są uje Jak Jak Jak uje się uje Jeuje się się się Jak Jak uje Jak Jak się Jak Miaby Jak się się mnie\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000004e-05\n",
      "std      4.456678e-03\n",
      "min      2.128197e-09\n",
      "25%      1.769387e-08\n",
      "50%      2.906483e-08\n",
      "75%      5.120774e-08\n",
      "max      9.965535e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 1.5681272745132446\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, do da ogonem. ósmej uważać nich Wtorek ósmej widzi się Wtorek uje ósmej uje ósmej kota ował masz Jak Jak uje uje Jak Wtorek Jak się kota Jak Jak Jak Jak Jak Jak siebie szył Jak blisko Znak raz Jak szył kota\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000003e-05\n",
      "std      4.459394e-03\n",
      "min      1.178548e-09\n",
      "25%      1.288535e-08\n",
      "50%      2.162905e-08\n",
      "75%      3.940991e-08\n",
      "max      9.971608e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 1.3176002502441406\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno może wody do kąpieli z kosze. Kiedy luKiedy ósmej pokazuje Jak kierunku mogę ósmej uje Kiedy mogę Jak czy Tak Wtorek ósmej górę się szył Jak górę one Wtorek szył ósmej mogę Kiedy Jak Kiedy Kiedy szył się\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000002e-05\n",
      "std      4.463116e-03\n",
      "min      5.439750e-10\n",
      "25%      6.479788e-09\n",
      "50%      1.104131e-08\n",
      "75%      2.062083e-08\n",
      "max      9.979930e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 1.1426523923873901\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. porządku się widzi górę nich się się Jak szył przechodzi raz się się raz raz raz pokazuje Jak się górę uje się Jak Wtorek się Jak Wtorek szył się Wtorek szył się szył ka Jak Wtorek się się ósmej raz się się chyba się\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.465024e-03\n",
      "min      3.809544e-10\n",
      "25%      4.449287e-09\n",
      "50%      7.633300e-09\n",
      "75%      1.445527e-08\n",
      "max      9.984198e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 1.0356847047805786\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. akwarsię! raz chłopiec uje się się chłopiec się uje dół deszcz k uje uje Kiedy czwartego czwartego czwartego czwartego wszyscy Wtorek Wtorek Wtorek Jak uje Wtorek górę Jak raz Wtorek blisko chłopiec Wtorek Babcia raz raz się\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.464715e-03\n",
      "min      4.259833e-10\n",
      "25%      4.925045e-09\n",
      "50%      8.417230e-09\n",
      "75%      1.606995e-08\n",
      "max      9.983506e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.9302058815956116\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno siódwody do kąpieli z wskaz. raz ósmej szył Jeleń trójraz kota kota uje ósmej ósmej siebie przechodzi przechodzi Wtorek Misię wszyscy się Jak Wtorek chJak szył ował ósmej szył Mimogę szył się się coś\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.467131e-03\n",
      "min      2.391910e-10\n",
      "25%      2.911775e-09\n",
      "50%      5.096888e-09\n",
      "75%      9.921436e-09\n",
      "max      9.988908e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.7791839241981506\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. lusię raz Miszył deszcz czwartego raz ósmej dą raz siebie się ś szył perkuszył Miósmej pokazuje raz blisko Kiedy się Dziwne się się szył się się Jak Jak Jak się szył Kiedy Kiedy Kiedy Jak Kiedy szył Kiedy Kiedy się\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.467711e-03\n",
      "min      1.985321e-10\n",
      "25%      2.187284e-09\n",
      "50%      3.793054e-09\n",
      "75%      7.323812e-09\n",
      "max      9.990206e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.7319203019142151\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. porządku pokazuje kota się ś Dziwne porządku Dziwne wszyscy deszcz raz przechodzi porządku trójDziwne uje przechodzi trójWtorek Wtorek widzi porządku przechodzi się Jak się Jak porządku dą siebie Kiedy chłopiec Wtorek złapał bierze Wtorek przechodzi szył szył\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.467557e-03\n",
      "min      2.087583e-10\n",
      "25%      2.322375e-09\n",
      "50%      4.061893e-09\n",
      "75%      8.020419e-09\n",
      "max      9.989861e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.6599286198616028\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza. kota kota Dziwne Miporządku raz luósmej MiMiKiedy Kiedy się poszli się przechodzi Wtorek przechodzi Wtorek się przechodzi Kiedy Kiedy porządku rynczwartego szył Kiedy szył Znak Kiedy Kiedy przechodzi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.468023e-03\n",
      "min      1.396207e-10\n",
      "25%      1.736843e-09\n",
      "50%      3.069250e-09\n",
      "75%      6.095171e-09\n",
      "max      9.990903e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.5599091053009033\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. raz raz się Kiedy Wtorek chszył się deszcz szył raz Miraz trójraz raz wszyscy raz Kiedy Wtorek ty Jak co Wtorek Jak Jak ował się się się chWtorek Wtorek Kiedy Jak Jak szył wiraz raz szył szył szył szył\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.469221e-03\n",
      "min      7.221546e-11\n",
      "25%      1.095086e-09\n",
      "50%      1.963983e-09\n",
      "75%      3.958307e-09\n",
      "max      9.993584e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.5286969542503357\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. przechodzi aby trójWtorek ludeszcz u porządku raz widzi Miraz nął czyli czwartego Wtorek Wtorek Wtorek Dlatego czwartego Wtorek położył Wtorek przechodzi Wtorek czwartego Wtorek Kiedy Jak siebie szył Wtorek przechodzi Wtorek wszyscy Kiedy się raz szył\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.469453e-03\n",
      "min      8.010069e-11\n",
      "25%      9.787056e-10\n",
      "50%      1.766250e-09\n",
      "75%      3.584312e-09\n",
      "max      9.994102e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.4640517830848694\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza. Może szył luraz raz raz widzi akwarraz porządku Wtorek deszcz nich deszcz czyli Wtorek porządku Wtorek się chraz się szył przechodzi szył przechodzi przechodzi by się raz szył szył Znak\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.469685e-03\n",
      "min      4.978369e-11\n",
      "25%      6.664680e-10\n",
      "50%      1.207620e-09\n",
      "75%      2.454997e-09\n",
      "max      9.994621e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.40239113569259644\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. luprzechodzi ósmej się stoją wszyscy raz kierunku raz kierunku coś coś raz ś ś pokazuje ło Kiedy ś Kiedy Kiedy raz luKiedy siebie przechodzi raz wszyscy coś się wszyscy picia Kiedy szył przechodzi chłopiec raz Wtorek ś coś Kiedy szył tyś\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.469542e-03\n",
      "min      5.870197e-11\n",
      "25%      6.898426e-10\n",
      "50%      1.246069e-09\n",
      "75%      2.561257e-09\n",
      "max      9.994300e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.4034537672996521\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. porządku trójprzechodzi babci Jeleń trójstoją kierunku Jeleń się porządku Jeleń raz deszcz słoraz przechodzi chłopiec Jeleń coś Wtorek chłopiec szył się chWtorek przechodzi przechodzi przechodzi wszyscy Jeleń raz chłopiec czyli by się się coś się\n",
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.469955e-03\n",
      "min      4.520172e-11\n",
      "25%      7.200695e-10\n",
      "50%      1.306153e-09\n",
      "75%      2.706438e-09\n",
      "max      9.995224e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.33391934633255005\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza. deszcz porządku skreślraz deszcz deszcz porządku porządku nął porządku Kiedy Wtorek stoją deszcz coś Jeleń porządku Wtorek przechodzi przechodzi Obok wieprzechodzi wieWtorek przechodzi przechodzi Kiedy się Kiedy raz Znak szył\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.470340e-03\n",
      "min      2.348970e-11\n",
      "25%      3.482010e-10\n",
      "50%      6.397056e-10\n",
      "75%      1.338756e-09\n",
      "max      9.996086e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.2934947907924652\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. raz kierunku picia coś stoją chłopiec przechodzi przechodzi ś deszcz ś raz stoją ś im ło ś deszcz położył Kiedy przechodzi zydotyka się ło ucieka Wtorek się szył wszyscy ka zypołożył szył wszyscy szył szył wiszył pływatytał wszyscy ś\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     1.999999e-05\n",
      "std      4.470154e-03\n",
      "min      3.012039e-11\n",
      "25%      3.931726e-10\n",
      "50%      7.205567e-10\n",
      "75%      1.505992e-09\n",
      "max      9.995669e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.2990601360797882\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. się trójś coś kierunku przechodzi raz przechodzi trójwyjeżdżchłopiec szył deszcz Kiedy Kiedy Kuwidzi Dlatego Kiedy Jeleń poszli Kiedy Jeleń dotyka się wszyscy dotyka ś Kiedy szył przechodzi szył Jeleń się się Kiedy raz się się\n",
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.470370e-03\n",
      "min      2.447870e-11\n",
      "25%      3.688721e-10\n",
      "50%      6.788479e-10\n",
      "75%      1.429342e-09\n",
      "max      9.996151e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.2518574595451355\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza. porządku stoją trójdeszcz czyli się dą raz Jeleń dź stoją stoją dź picia dotyka się przechodzi pokazuje chłopiec przechodzi chwszyscy chKiedy Kiedy Wtorek Kiedy przechodzi Babcia pokazuje Jeleń szesnastraz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.470550e-03\n",
      "min      2.787447e-11\n",
      "25%      3.968600e-10\n",
      "50%      7.300793e-10\n",
      "75%      1.530864e-09\n",
      "max      9.996554e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.2262566238641739\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. kierunku raz deszcz porządku przechodzi wszyscy ś ś wszyscy ś raz kierunku ś ś ś ś Wtorek raz ś szył raz chłopiec szył dotyka trójdeszcz szył Wtorek porządku Wtorek dotyka dotyka siebie ś raz przechodzi szył szył ławki wszyscy picia ty picia picia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000002e-05\n",
      "std      4.470951e-03\n",
      "min      1.566318e-11\n",
      "25%      2.201084e-10\n",
      "50%      4.105764e-10\n",
      "75%      8.821185e-10\n",
      "max      9.997451e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.22238093614578247\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. porządku uje kierunku Jeleń Jeleń pali się zaczęli się dem Jeleń Jeleń Jeleń Jeleń przydobry ludotyka ych przechodzi przechodzi fartuował pokazuje się się dotyka wszyscy ś raz Wtorek szył akwarJeleń pomyślał tywieJeleń się\n",
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.470712e-03\n",
      "min      1.594151e-11\n",
      "25%      2.378283e-10\n",
      "50%      4.412426e-10\n",
      "75%      9.394853e-10\n",
      "max      9.996915e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.17130741477012634\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza. dostrzega Kiedy Jeleń raz deszcz pali czwartego deszcz ś pełna przechodzi Kiedy Kiedy skreśldź deszcz się się się chKiedy raz pomyślał ś raz raz Wtorek raz czyli Kiedy raz ś raz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.470960e-03\n",
      "min      7.986324e-12\n",
      "25%      1.471131e-10\n",
      "50%      2.778889e-10\n",
      "75%      5.996524e-10\n",
      "max      9.997471e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.16027948260307312\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. uje raz raz typali przechodzi babci raz raz ś aby kierunku prostoś raz ry picia deszcz deszcz coś raz trójchłopiec fartudeszcz przechodzi Jeleń Jeleń kierunku ś chłopiec ty przechodzi Chcę Wtorek szył szył ś przechodzi wszyscy wszyscy wszyscy picia raz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.471133e-03\n",
      "min      1.334556e-11\n",
      "25%      1.867364e-10\n",
      "50%      3.471229e-10\n",
      "75%      7.393339e-10\n",
      "max      9.997858e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.1570032387971878\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. Jeleń wietablestoją dostrzega wieraz stoją Jeleń wiepokazuje Jeleń widzi nić nić ławki liczy wieliczy dą Jeleń Jeleń Jeleń przechodzi ś Wtorek ś ś stoją szybę przechodzi Jeleń ś przechodzi Babcia się Obok raz przechodzi\n",
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.471011e-03\n",
      "min      6.815149e-12\n",
      "25%      1.110941e-10\n",
      "50%      2.115723e-10\n",
      "75%      4.675870e-10\n",
      "max      9.997585e-01\n",
      "dtype: float64\n",
      "Iteration 0 : 0.11692168563604355\n",
      "Polish Annotation: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza.\n",
      "Prediction: Ja myślę, że na przykład nie wolno nosić wody do kąpieli z morza. przychodzić prostoraz się porządku ych siedemnastraz się pełna siebie dą przechodzi dotyka się się przechodzi ś się się raz przechodzi raz raz Jeleń luJeleń przechodzi raz przechodzi stoją szesnastś\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000000e-05\n",
      "std      4.471255e-03\n",
      "min      5.580550e-12\n",
      "25%      1.202851e-10\n",
      "50%      2.280922e-10\n",
      "75%      4.955188e-10\n",
      "max      9.998129e-01\n",
      "dtype: float64\n",
      "Iteration 100 : 0.10740341991186142\n",
      "Polish Annotation: Policjant zastanawia się.\n",
      "Prediction: Policjant zastanawia się. kierunku odleluraz wieraz k raz Wtorek uważają mogę deszcz coś ś ś ś pali ś wszyscy pali ś ś ś ś deszcz wszyscy t górę ś ś się Wtorek szkoda deszcz pali szył ś raz Zastanawiam spada biec przechodzi przechodzi ś\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51506/2702555863.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=count    5.000000e+04\n",
      "mean     2.000001e-05\n",
      "std      4.471345e-03\n",
      "min      6.254172e-12\n",
      "25%      1.111738e-10\n",
      "50%      2.115845e-10\n",
      "75%      4.633922e-10\n",
      "max      9.998331e-01\n",
      "dtype: float64\n",
      "Iteration 200 : 0.11461439728736877\n",
      "Polish Annotation: Kot idzie zadowolony, merda ogonem.\n",
      "Prediction: Kot idzie zadowolony, merda ogonem. wszyscy wieś Jeleń raz Jeleń ś stoją Jeleń moniJeleń raz dostrzega pali Jeleń Niedziela bagażprzechodzi nić Jeleń bloperkuprzechodzi Jeleń Wtorek tableka picia ś Jeleń Jeleń Jeleń Jeleń Obok ś Obok Wtorek pomyślał szył\n"
     ]
    }
   ],
   "source": [
    "transformer.train()\n",
    "transformer.to(DEVICE)\n",
    "\n",
    "metrics_history = []\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    for batch_num, batch in enumerate(train_dl):\n",
    "        transformer.train()\n",
    "        x, y = batch[\"in_landmarks\"].to(DEVICE), batch[\"out_polish_token_ids\"].to(DEVICE)\n",
    "        # print(f\"{x.shape=}, {y.shape=}\")\n",
    "\n",
    "        encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask = create_masks(x, y)\n",
    "        optim.zero_grad()\n",
    "\n",
    "        prediction = transformer(x, y, encoder_self_attention_mask.to(DEVICE),  decoder_self_attention_mask.to(DEVICE), decoder_cross_attention_mask.to(DEVICE))\n",
    "        # print(f\"{ids_predictions.shape=} {ids_predictions.dtype=}\")\n",
    "\n",
    "        criterion_input = prediction.view(-1, VOCAB_SIZE)\n",
    "        # print(f\"{criterion_input.shape=} {criterion_input.dtype=}\")\n",
    "\n",
    "        criterion_target = y.view(-1).long()\n",
    "        # print(f\"{criterion_target.shape=} {criterion_target.dtype=}\")\n",
    "\n",
    "        loss = criterion(criterion_input, criterion_target)\n",
    "\n",
    "        valid_indicies = torch.where(criterion_target.view(-1) == tokenizer.pad_token_id, False, True)\n",
    "        loss = loss.sum() / valid_indicies.sum()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        metrics_history.append({\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"TrainLoss\": loss.item(),\n",
    "        })\n",
    "\n",
    "        if batch_num % 100 == 0:\n",
    "            y_batch_decoded = tokenizer.batch_decode(y, skip_special_tokens=True)\n",
    "            print(f\"{pd.Series(F.softmax(prediction[0][0]).detach().cpu()).describe()=}\")\n",
    "            ids_predictions = torch.argmax(prediction, dim=-1)\n",
    "            # thresholded_predictions = torch.where(prediction > TOKEN_PROB_THRESHOLD, 1, 0)\n",
    "            pred_decoded = tokenizer.batch_decode(ids_predictions, skip_special_tokens=True)\n",
    "            print(f\"Iteration {batch_num} : {loss.item()}\")\n",
    "            print(f\"Polish Annotation: {y_batch_decoded[0]}\")\n",
    "            print(f\"Prediction: {pred_decoded[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epoch</th>\n",
       "      <th>TrainLoss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10.904820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9.461005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>9.158460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>8.537040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>8.279226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>10</td>\n",
       "      <td>0.094989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>10</td>\n",
       "      <td>0.083586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>10</td>\n",
       "      <td>0.076297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>10</td>\n",
       "      <td>0.051895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>10</td>\n",
       "      <td>0.020253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2450 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Epoch  TrainLoss\n",
       "0         1  10.904820\n",
       "1         1   9.461005\n",
       "2         1   9.158460\n",
       "3         1   8.537040\n",
       "4         1   8.279226\n",
       "...     ...        ...\n",
       "2445     10   0.094989\n",
       "2446     10   0.083586\n",
       "2447     10   0.076297\n",
       "2448     10   0.051895\n",
       "2449     10   0.020253\n",
       "\n",
       "[2450 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_df = pd.DataFrame.from_records(metrics_history)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Epoch', ylabel='TrainLoss'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(data=metrics_df, x=\"Epoch\", y=\"TrainLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(transformer.state_dict(), MODELS_DIR / \"transformer_v1.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(\n",
    "    d_model=D_MODEL,\n",
    "    ffn_hidden=FFN_HIDDEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    drop_prob=DROP_PROB,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 512])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn( (BATCH_SIZE, MAX_SEQUENCE_LENGTH, N_LANDMARKS) ).to(DEVICE) # includes positional encoding\n",
    "out = encoder(x, self_attention_mask=None)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn( (BATCH_SIZE, MAX_SEQUENCE_LENGTH, D_MODEL) ).to(DEVICE)  # seq of frames (landmarks), batched; positional encoded\n",
    "y = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_SEQUENCE_LENGTH) ).to(DEVICE)  # batched tokens ids; positional encoded\n",
    "\n",
    "mask = torch.full([MAX_SEQUENCE_LENGTH, MAX_SEQUENCE_LENGTH] , float('-inf')).to(DEVICE)\n",
    "mask = torch.triu(mask, diagonal=1).to(DEVICE)\n",
    "\n",
    "decoder = Decoder(D_MODEL, FFN_HIDDEN, NUM_HEADS, DROP_PROB, NUM_LAYERS, MAX_SEQUENCE_LENGTH, VOCAB_SIZE).to(DEVICE)\n",
    "\n",
    "out = decoder(x, y, mask, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 512])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    d_model=D_MODEL,\n",
    "    ffn_hidden=FFN_HIDDEN,\n",
    "    num_heads=NUM_HEADS,\n",
    "    drop_prob=DROP_PROB,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn( (BATCH_SIZE, MAX_SEQUENCE_LENGTH, N_LANDMARKS) ).to(get_device())\n",
    "y = torch.randint(0, VOCAB_SIZE, (BATCH_SIZE, MAX_SEQUENCE_LENGTH) ).to(get_device())\n",
    "\n",
    "result = transformer(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50, 50000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.9627,  0.0483, -0.8679,  ...,  0.1195, -0.1094,  0.6290],\n",
       "         [ 0.9919,  0.0301, -0.3626,  ...,  0.4176, -0.1630,  0.6565],\n",
       "         [ 0.7686, -0.1704,  0.1748,  ...,  0.5258, -1.4025,  0.5030],\n",
       "         ...,\n",
       "         [ 0.7538,  0.4849, -0.0271,  ...,  0.6694, -0.9478,  0.5285],\n",
       "         [ 1.4585, -0.0435,  0.2559,  ...,  0.3148, -0.3352,  0.6046],\n",
       "         [ 1.2859, -0.6242, -0.3211,  ...,  0.3898, -0.2568,  0.9221]],\n",
       "\n",
       "        [[ 0.3515, -0.5566,  0.1684,  ...,  0.4672, -0.3015,  0.6946],\n",
       "         [ 0.2413,  0.2286,  0.3218,  ...,  0.4733, -0.5607,  0.3345],\n",
       "         [-0.6346,  0.3005,  0.4521,  ...,  0.2904, -0.2683, -0.0039],\n",
       "         ...,\n",
       "         [ 1.1504,  0.6891,  0.5858,  ...,  0.1815, -0.2292,  1.2092],\n",
       "         [ 0.3794,  0.1834,  1.0685,  ..., -0.5546, -0.6466,  0.8908],\n",
       "         [ 0.7399, -0.4228,  0.7433,  ..., -0.5248, -0.4261,  0.9464]],\n",
       "\n",
       "        [[ 0.2302, -0.3049,  0.0202,  ..., -0.2127, -0.0866,  0.4929],\n",
       "         [ 0.9873, -0.2797, -0.7383,  ...,  0.1519,  0.1445, -0.2131],\n",
       "         [-0.1378, -0.1727, -0.4946,  ..., -1.2139, -0.3748,  0.3109],\n",
       "         ...,\n",
       "         [-0.1454,  0.0131,  0.1265,  ..., -0.4625, -0.1370,  0.3924],\n",
       "         [ 0.2196,  0.1182, -0.5297,  ..., -0.2155, -0.6738,  0.0645],\n",
       "         [ 0.3750, -0.2024, -0.3065,  ..., -0.5555,  0.3477,  1.2354]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.2955,  0.5402,  0.2496,  ..., -0.5783,  0.0545,  1.2552],\n",
       "         [ 0.1350,  0.1505,  0.4742,  ..., -0.4938, -0.0130,  0.3638],\n",
       "         [-0.6198,  0.3269, -0.3388,  ..., -0.4388, -0.9352, -0.2529],\n",
       "         ...,\n",
       "         [ 0.8423, -0.0826, -0.0023,  ...,  0.0031, -0.0271,  0.1231],\n",
       "         [ 0.6491,  0.6096, -0.0604,  ...,  0.5091,  0.0629,  1.0291],\n",
       "         [ 0.3111,  0.5954,  0.3220,  ...,  0.5200,  0.4188,  0.8646]],\n",
       "\n",
       "        [[ 0.1283,  1.0767, -0.2139,  ...,  0.3393, -0.3628,  0.6403],\n",
       "         [ 0.4363, -0.4464, -0.5586,  ...,  0.5151, -0.1203,  0.1716],\n",
       "         [ 0.2657,  0.1674,  0.2283,  ..., -0.4846, -0.2535,  0.0703],\n",
       "         ...,\n",
       "         [ 0.2090,  0.1905, -0.3601,  ..., -0.2685,  0.0966,  0.6247],\n",
       "         [ 0.1762,  0.2879, -0.1387,  ...,  0.4893, -0.2986,  0.6477],\n",
       "         [ 0.0840, -0.2398, -0.2616,  ..., -0.3866, -0.4232,  0.6934]],\n",
       "\n",
       "        [[ 0.4628,  0.6163,  0.3957,  ...,  0.1976, -0.4224,  0.5808],\n",
       "         [-0.3005,  0.6594,  0.1061,  ...,  0.0194, -0.7178,  1.0113],\n",
       "         [ 0.5925, -0.0263, -0.4851,  ...,  0.1753, -0.5518,  0.6798],\n",
       "         ...,\n",
       "         [ 0.5380, -0.1816,  0.2512,  ...,  0.0598,  0.2352,  1.2621],\n",
       "         [ 0.7149,  0.6473,  0.3348,  ...,  0.6328, -0.6345, -0.2003],\n",
       "         [ 0.4973, -0.4273, -0.0854,  ..., -0.3257, -0.2317,  0.6144]]],\n",
       "       device='cuda:0', grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
